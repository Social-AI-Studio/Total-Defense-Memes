{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f8c1dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a3a914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39989"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# google search images\n",
    "len(glob2.glob(\"/mnt/sda/dataset/google_search_results/batch1/original/*/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa798fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39989 files copied successfully!, out of 39989\n",
      "Following files not copied:\n",
      "\n",
      "\n",
      "Following files did not match pillar:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy and rename google search files\n",
    "success,not_found,errors,file_map_google=copy_map_and_rename(glob2.glob(\"/mnt/sda/dataset/google_search_results/batch1/original/*/*\"),\\\n",
    "     \"/mnt/sda/dataset/google_insta_reddit/original\")\n",
    "\n",
    "print(\"{} files copied successfully!, out of {}\".format(success,\\\n",
    "            len(glob2.glob(\"/mnt/sda/dataset/google_search_results/batch1/original/*/*\"))))\n",
    "print(\"Following files not copied:\\n\" + \"\\n\".join(errors))\n",
    "print()\n",
    "print(\"Following files did not match pillar:\\n\" + \"\\n\".join(not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff80dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38755\n"
     ]
    }
   ],
   "source": [
    "# Instagram files\n",
    "print(len(glob2.glob(\"/mnt/sda/abdul/rum/data-scrapping/data/instagram/*/*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1ed5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33546"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File names are instagram ids, getting total unique file count\n",
    "len(set([file.split(\"/\")[-1] for file in glob2.glob(\"/mnt/sda/abdul/rum/data-scrapping/data/instagram/*/*\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535374ee",
   "metadata": {},
   "source": [
    "38755-33546=5209 duplicate file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d904b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33546 files copied successfully!, out of 33546\n",
      "Following files not copied:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy and rename insta files\n",
    "unique_file_names = get_unique_filenames(glob2.glob(\"/mnt/sda/abdul/rum/data-scrapping/data/instagram/*/*\"))       \n",
    "success,errors,file_map_insta=copy_and_rename(unique_file_names,\"/mnt/sda/dataset/google_insta_reddit/original\",\"insta\")\n",
    "print(\"{} files copied successfully!, out of {}\".format(success,len(unique_file_names)))\n",
    "print(\"Following files not copied:\\n\" + \"\\n\".join(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3c6ab7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73535, 73535)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tally\n",
    "33546+39989, len(glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/original/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b425fd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  238037\n",
      "unique file names:  237880\n",
      "237880 files copied successfully!, out of 237880\n",
      "Following files not copied:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reddit files\n",
    "print(\"total: \",len(glob2.glob(\"/mnt/sda/abdul/rum/data-scrapping/data/reddit/*/images/*\")))\n",
    "print(\"unique file names: \",len(set([file.split(\"/\")[-1] for file in glob2.glob(\"/mnt/sda/abdul/rum/data-scrapping/data/reddit/*/images/*\")])))\n",
    "unique_file_names=get_unique_filenames(glob2.glob(\"/mnt/sda/abdul/rum/data-scrapping/data/reddit/*/images/*\"))\n",
    "success,errors,file_map_reddit=copy_and_rename(unique_file_names,\"/mnt/sda/dataset/google_insta_reddit/original\",\"reddit\")\n",
    "print(\"{} files copied successfully!, out of {}\".format(success,len(unique_file_names)))\n",
    "print(\"Following files not copied:\\n\" + \"\\n\".join(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d84651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311415, 311415)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tally \n",
    "33546+39989+237880, len(glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/original/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dafc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_map={}\n",
    "file_map.update(file_map_google)\n",
    "file_map.update(file_map_insta)\n",
    "file_map.update(file_map_reddit)\n",
    "with open('/mnt/sda/dataset/google_insta_reddit/postprocessing/filemap.json','w') as f:\n",
    "    json.dump(file_map,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6ad0e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non corrupted: 282682 out of 311415\n"
     ]
    }
   ],
   "source": [
    "# removing corrupted files\n",
    "non_corrupted=return_non_corrupted_files(\"/mnt/sda/dataset/google_insta_reddit/original/\")\n",
    "non_corrupted_files=[item[0] for item in non_corrupted]\n",
    "print(\"non corrupted: {} out of {}\".format(len(non_corrupted),\\\n",
    "                                          len(glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/original/*\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ae7e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_corrupted_dict={}\n",
    "for item in non_corrupted:\n",
    "    non_corrupted_dict[item[0]]=item[1]\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/postprocessing/non_corrupted.json\",\"w\") as f:\n",
    "    json.dump(non_corrupted_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac9bb4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282682 files copied successfully!, out of 282682\n"
     ]
    }
   ],
   "source": [
    "success,errors=copy(non_corrupted_files,\"/mnt/sda/dataset/google_insta_reddit/postprocessing/non_corrupted\")\n",
    "print(\"{} files copied successfully!, out of {}\".format(success,len(non_corrupted_files)))\n",
    "# print(\"Following files not copied:\\n\" + \"\\n\".join(errors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "979c5516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122858\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (90414000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194050\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241922\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (135473130 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267854 text extracted !, out of 311416\n",
      "file count without text: 15681\n",
      "file count extracted text: 267854/311416\n",
      "error file count: 27881\n"
     ]
    }
   ],
   "source": [
    "# removing no text\n",
    "# reader = easyocr.Reader(['en'], gpu=[\"cuda:0\",\"cuda:1\"])\n",
    "files=glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/original/*\")\n",
    "ocr_text,error_files,no_text_files=read_txt(files)\n",
    "print(\"{} text extracted !, out of {}\".format(len(ocr_text),len(files)))\n",
    "print(\"file count without text: {}\".format(len(no_text_files)))\n",
    "print(\"file count extracted text: {}/{}\".format(len(ocr_text),len(files)))\n",
    "print(\"error file count: {}\".format(len(error_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438f4e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311416"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tally\n",
    "15681+27881+267854 #(1 extra file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91984703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample output  [([[16, 12], [52, 12], [52, 38], [16, 38]], '2', 0.0674939737582605), ([[76, 3], [472, 3], [472, 33], [76, 33]], 'National Crime Prevention Council Singapore', 0.9937172669946763), ([[77, 33], [131, 33], [131, 53], [77, 53]], '4d*6', 0.8501265645027161), ([[14, 62], [792, 62], [792, 93], [14, 93]], \"Don t be fooled by those claiming to be who they' re not; offering services they can t fulfil; or\", 0.5519198740110262), ([[14, 90], [500, 90], [500, 118], [14, 118]], 'attempting to emotionally blackmail you for your money:', 0.6397081144193957), ([[12, 124], [700, 124], [700, 152], [12, 152]], 'Scammers are Still on & spree; stay cautious and protect your hard-earned money:', 0.46274645699028855), ([[14, 160], [278, 160], [278, 186], [14, 186]], 'Spot the signs; stop the crimes:', 0.7692638893263821), ([[14, 186], [354, 186], [354, 212], [14, 212]], '#spotthesigns #spreadthewordonscams', 0.9663249112814105), ([[75, 263], [345, 263], [345, 303], [75, 303]], 'MY DOG STEPPED ON A BEE', 0.9250856951921306), ([[501, 263], [704, 263], [704, 304], [501, 304]], 'THEY WANT MY OTP', 0.5925277203683613)]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/sda/dataset/google_insta_reddit/postprocessing/ocr.json\",\"w\") as f:\n",
    "    json.dump(str(ocr_text),f)\n",
    "print(\"sample output \",ocr_text[list(ocr_text.keys())[0]]) \n",
    "ocr_text_filtered={}\n",
    "for file,texts in ocr_text.items():\n",
    "    ocr_text_filtered[file]=\"\\n\".join([text[1] for text in texts])\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/postprocessing/ocr_text_only.json\",\"w\") as f:\n",
    "    json.dump(str(ocr_text_filtered),f) \n",
    "      \n",
    "# to csv\n",
    "pd.DataFrame(ocr_text_filtered.items(),columns=['file','text']).replace(regex = '\\n', value = ' ').\\\n",
    "    to_csv('/mnt/sda/dataset/google_insta_reddit/postprocessing/ocr_text.csv',index=False)\n",
    "# to json\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/postprocessing/ocr_text_only.json\",\"w\") as f:\n",
    "    json.dump(ocr_text_filtered,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0b0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/sda/dataset/google_insta_reddit/postprocessing/filemap.json','r') as f:\n",
    "    file_map=json.load(f)      \n",
    "file_map_old_to_new={old:new for new,old in file_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with text: 267854 out of 282682\n",
      "index: 261717\r"
     ]
    }
   ],
   "source": [
    "# copy files\n",
    "\n",
    "with_text=[]\n",
    "for index,(file,_) in enumerate(non_corrupted_dict.items()):\n",
    "    file_name=file.split(\"/\")[-1]\n",
    "    if file_name in ocr_text_filtered:\n",
    "        with_text.append(file_name)\n",
    "print(\"with text: {} out of {}\".format(len(with_text),len(non_corrupted_dict)))\n",
    "\n",
    "# save with text files\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/postprocessing/with_text.txt\",\"w\") as f:\n",
    "    f.write(\"\\n\".join(with_text))\n",
    "    \n",
    "success=0\n",
    "for index,file_name in enumerate(with_text):\n",
    "    file_path=\"/mnt/sda/dataset/google_insta_reddit/postprocessing/non_corrupted/{}\".\\\n",
    "                format(file_name)\n",
    "    shutil.copy(file_path,\\\n",
    "            \"/mnt/sda/dataset/google_insta_reddit/postprocessing/with_text\")\n",
    "    success+=1\n",
    "    print(\"index: {}\".format(index),end=\"\\r\")  \n",
    "print(\"copied {} out of {}\".format(success,\\\n",
    "            len(glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/postprocessing/non_corrupted/*\"))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc889b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231194\n",
      "psychological dups: 1694\n",
      "digital dups: 3190\n",
      "reddit dups: 9451\n",
      "civil dups: 1603\n",
      "social dups: 8908\n",
      "others dups: 3204\n",
      "military dups: 2929\n",
      "economic dups: 3106\n",
      "insta dups: 2575\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "duplicates=return_duplicates('/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/with_text/')\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/duplicate_output.json\",\"w\") as f:\n",
    "    json.dump(duplicates,f)\n",
    "    \n",
    "# get unique samples\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/ocr_text_only.json\",\"r\") as f:\n",
    "    ocr_text_filtered=json.load(f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/non_corrupted.json\",\"r\") as f:\n",
    "    non_corrupted_dict=json.load(f)\n",
    "    \n",
    "unique_samples=get_unique_samples(duplicates,ocr_text_filtered,\\\n",
    "                                  {key.split(\"/\")[-1]:size for key,size in non_corrupted_dict.items()})\n",
    "print(len(unique_samples))\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/unique_samples.txt\",\"w\") as f:\n",
    "    f.write(\"\\n\".join(unique_samples))\n",
    "    \n",
    "# class-wise duplicates\n",
    "dups=[file.split(\"_\")[0] for file in list(duplicates.keys()) if file not in unique_samples]\n",
    "for item in set(dups):\n",
    "    print(\"{} dups: {}\".format(item,dups.count(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa2d29cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267854, 267854)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tally\n",
    "231194+1694+3190+9451+1603+8908+3204+2929+3106+2575,267854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a17dc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/unique_samples.txt\",\"r\") as f:\n",
    "    unique_samples=f.read().split(\"\\n\")\n",
    "with open('/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/filemap.json','r') as f:\n",
    "    file_map=json.load(f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/ocr_text_only.json\",\"r\") as f:\n",
    "    ocr_text_filtered=json.load(f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/non_corrupted.json\",\"r\") as f:\n",
    "    non_corrupted_dict=json.load(f)\n",
    "non_corrupted_dict={file.split(\"/\")[-1]:size for file,size in non_corrupted_dict.items()}    \n",
    "ocr_text_filtered={file:\" \".join(text.split(\"\\n\")) for file,text in ocr_text_filtered.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "723c4308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top priority:  1371\n",
      "all vocab:  17732 top vocab:  3546\n",
      "medium priority:  10756\n",
      "low priority:  3662\n",
      "least priority:  16497\n"
     ]
    }
   ],
   "source": [
    "# Identifying priority\n",
    "top_priority_keywords=['CPF','HDB','NSF','National Service','SPF','newater','new water',\\\n",
    "                      'scdf','tracetogether','east coast','Heng Swee','Lee Hsing','smart nation',\\\n",
    "                      'sinkies','SAF','encik','chinese','indian','ndp','fresh chicken','memedef','NS']\n",
    "# ocr_text_filtered={file:\" \".join(text.split(\"\\n\")) for file,text in ocr_text_filtered.items()}\n",
    "google_insta_text=[(file,ocr_text_filtered[file]) for file in unique_samples if \"reddit_\" not in file]\n",
    "filtered_top_priority=[file for file,text in google_insta_text if any([token.lower() in text.lower().split(\" \")\\\n",
    "                                                                       for token in top_priority_keywords])]\n",
    "print(\"top priority: \",len(filtered_top_priority))\n",
    "\n",
    "# get vocab from google searched memes\n",
    "google_text=[ocr_text_filtered[file] for file in unique_samples if \"insta_\" not in file and \"reddit_\" not in file]\n",
    "vect = CountVectorizer()\n",
    "bow = vect.fit_transform(google_text)\n",
    "total_features = len(vect.vocabulary_)\n",
    "top_vect = CountVectorizer(max_features=int(total_features * 0.2))\n",
    "top_bow = top_vect.fit_transform(google_text)\n",
    "print(\"all vocab: \",len(vect.vocabulary_),\"top vocab: \",len(top_vect.vocabulary_))\n",
    "\n",
    "# remove common english words from vocab\n",
    "common_eng_wrods=pd.read_csv(\"/mnt/sda/nirmal/rum/words_pos.csv\")['word'].tolist()\n",
    "useful_tokens=['singapore']+\\\n",
    "[word.lower() for word in list(top_vect.vocabulary_.keys()) if word not in common_eng_wrods and word.isalpha()]\n",
    "\n",
    "google_insta_text=[(file,ocr_text_filtered[file]) for file in unique_samples if \"reddit\" not in file]\n",
    "filtered=[(file,text) for file,text in google_insta_text if any([token.lower() in text.lower().split(\" \")\\\n",
    "                                                                 for token in useful_tokens])]\n",
    "filtered_medium_priority=[file for file,text in filtered if file not in filtered_top_priority]\n",
    "print(\"medium priority: \",len(filtered_medium_priority))\n",
    "\n",
    "useful_tokens=['singapore']+\\\n",
    "[word.lower() for word in list(vect.vocabulary_.keys()) if word not in common_eng_wrods and word.isalpha()]\n",
    "filtered=[(file,text) for file,text in google_insta_text if any([token.lower() in text.lower().split(\" \")\\\n",
    "                                                                 for token in useful_tokens])]\n",
    "\n",
    "filtered_low_priority=[file for file,text in filtered if file not in filtered_top_priority and file not in \\\n",
    "                     filtered_medium_priority ]\n",
    "print(\"low priority: \",len(filtered_low_priority))\n",
    "\n",
    "# least priority - google,insta files not covered above\n",
    "filtered_least_priority=[file for file in unique_samples if file not in [*filtered_top_priority,\\\n",
    "                                *filtered_medium_priority,*filtered_low_priority] and \"reddit\" not in file]\n",
    "print(\"least priority: \",len(filtered_least_priority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3a3fcd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "\n",
    "original_names=[file_map[file] for file in unique_samples]\n",
    "texts=[ocr_text_filtered[file] for file in unique_samples]\n",
    "text_lengths=[len(text.split(\" \")) for text in texts]\n",
    "platform=[file.split(\"_\")[0] for file in unique_samples]\n",
    "google_search_map=glob2.glob(\"/mnt/sda/abdul/rum/data-scrapping/data/google_search_images/*/*\")\n",
    "google_search_map=[file.replace(\"/mnt/sda/abdul/rum/data-scrapping/data/google_search_images/\",\"\")\\\n",
    "                     for file in google_search_map]\n",
    "google_search_map={file.split(\"/\")[-1]:file.split(\"/\")[0] for file in google_search_map}\n",
    "source=[google_search_map[file_map[file]] if \"insta_\" not in file and \"reddit_\" not in file else \"\" \\\n",
    "        for file in unique_samples]\n",
    "platform=[\"instagram\" if \"insta_\" in file else \"reddit\" if \"reddit_\" in file else \"google\" \\\n",
    "          for file in unique_samples]\n",
    "keywords=[\",\".join([token for token in ocr_text_filtered[file].split(\" \") if token.lower() in \\\n",
    "                    list(map(lambda x: x.lower(), [*top_priority_keywords,*useful_tokens]))])\\\n",
    "          if file in [*filtered_top_priority,*filtered_medium_priority,*filtered_low_priority] \\\n",
    "          else \"\" for file in unique_samples]\n",
    "image_sizes=[non_corrupted_dict[file] for file in unique_samples]\n",
    "\n",
    " \n",
    "priority={\"file\":unique_samples,\"original file\":original_names,\"text\":texts,\"text length\":text_lengths,\\\n",
    "         \"platform\":platform,\"google search keywords\":source,\"priority keywords\":keywords,\"size\":image_sizes}\n",
    "                  \n",
    "priority=pd.DataFrame(priority)                  \n",
    "priority['priority']=''\n",
    "for i,row in priority.iterrows():\n",
    "    if(row['file'] in filtered_top_priority):\n",
    "        priority.at[i,'priority'] = 'top'\n",
    "    elif(row['file'] in filtered_medium_priority):\n",
    "        priority.at[i,'priority'] = 'medium'\n",
    "    elif(row['file'] in filtered_low_priority):\n",
    "        priority.at[i,'priority'] = 'low'\n",
    "    elif(row['file'] in filtered_least_priority):\n",
    "        priority.at[i,'priority'] = 'least'                  \n",
    "priority.to_csv('/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/priority.csv',\\\n",
    "                                  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3dea58",
   "metadata": {},
   "source": [
    "batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191f986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4104"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same file is downloaded multiple times, filtering them\n",
    "files=glob2.glob(\"/mnt/sda/nirmal/imageye/*/*\")\n",
    "uniques=set([file.split(\"/\")[-1].split(\"_\")[0] for file in files if (\"_\") in file and \"download\" not in file])\n",
    "len(uniques)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1db23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 3958)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the duplicate downloads, getting the max size file for each unique file\n",
    "# the web plugin downloads files with different resolution each time\n",
    "filtered=[]\n",
    "errored=[]\n",
    "for uniq in uniques:\n",
    "    duplicates=[file for file in files if uniq in file]\n",
    "    sizes=[]\n",
    "    for dup in duplicates:\n",
    "        try:\n",
    "            img = Image.open(dup)  # open the image file\n",
    "            img.verify()\n",
    "            sizes.append(img.size)\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            errored.append(uniq)\n",
    "            break\n",
    "    if uniq not in errored:     \n",
    "        max_size=max([sizes[0] for size in sizes])\n",
    "        filtered.append((max_size,duplicates[sizes.index(max_size)]))  \n",
    "len(errored), len(filtered)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3825859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3958 files copied successfully!, out of 3958\n",
      "Following files not copied:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy to batch2\n",
    "success,errors,file_map_insta=copy_and_rename([file for _,file in filtered],'/mnt/sda/dataset/google_insta_reddit/batch2/original',\\\n",
    "               scrape_source='insta',start_indx=33546)\n",
    "print(\"{} files copied successfully!, out of {}\".format(success,len(filtered)))\n",
    "print(\"Following files not copied:\\n\" + \"\\n\".join(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab11bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/filemap.json','w') as f:\n",
    "    json.dump(file_map_insta,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc3a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non corrupted: 3958 out of 3958\n"
     ]
    }
   ],
   "source": [
    "# removing corrupted files\n",
    "non_corrupted=return_non_corrupted_files(\"/mnt/sda/dataset/google_insta_reddit/batch2/original/\")\n",
    "non_corrupted_files=[item[0] for item in non_corrupted]\n",
    "print(\"non corrupted: {} out of {}\".format(len(non_corrupted),\\\n",
    "                                          len(glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/batch2/original/*\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8e864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_corrupted_dict={}\n",
    "for item in non_corrupted:\n",
    "    non_corrupted_dict[item[0]]=item[1]\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted.json\",\"w\") as f:\n",
    "    json.dump(non_corrupted_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98e0cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3958 files copied successfully!, out of 3958\n"
     ]
    }
   ],
   "source": [
    "success,errors=copy(non_corrupted_files,\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted\")\n",
    "print(\"{} files copied successfully!, out of {}\".format(success,len(non_corrupted_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd17a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3797 text extracted !, out of 3958\n",
      "file count without text: 161\n",
      "file count extracted text: 3797/3958\n",
      "error file count: 0\n"
     ]
    }
   ],
   "source": [
    "files=glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted/*\")\n",
    "ocr_text,error_files,no_text_files=read_txt(files)\n",
    "print(\"{} text extracted !, out of {}\".format(len(ocr_text),len(files)))\n",
    "print(\"file count without text: {}\".format(len(no_text_files)))\n",
    "print(\"file count extracted text: {}/{}\".format(len(ocr_text),len(files)))\n",
    "print(\"error file count: {}\".format(len(error_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0efa6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample output  [([[13, 130], [147, 130], [147, 150], [13, 150]], 'How Malaysia enters', 0.9974318162778757), ([[157, 131], [199, 131], [199, 147], [157, 147]], 'house', 0.9999468477543129)]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/ocr.json\",\"w\") as f:\n",
    "    json.dump(str(ocr_text),f)\n",
    "print(\"sample output \",ocr_text[list(ocr_text.keys())[0]]) \n",
    "ocr_text_filtered={}\n",
    "for file,texts in ocr_text.items():\n",
    "    ocr_text_filtered[file]=\"\\n\".join([text[1] for text in texts])\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/ocr_text_only.json\",\"w\") as f:\n",
    "    json.dump(str(ocr_text_filtered),f) \n",
    "      \n",
    "# to csv\n",
    "pd.DataFrame(ocr_text_filtered.items(),columns=['file','text']).replace(regex = '\\n', value = ' ').\\\n",
    "    to_csv('/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/ocr_text.csv',index=False)\n",
    "# to json\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/ocr_text_only.json\",\"w\") as f:\n",
    "    json.dump(ocr_text_filtered,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b3e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with text: 3797 out of 3958\n",
      "copied 3797 out of 3958\n"
     ]
    }
   ],
   "source": [
    "with_text=[]\n",
    "for index,(file,_) in enumerate(non_corrupted_dict.items()):\n",
    "    file_name=file.split(\"/\")[-1]\n",
    "    if file_name in ocr_text_filtered:\n",
    "        with_text.append(file_name)\n",
    "print(\"with text: {} out of {}\".format(len(with_text),len(non_corrupted_dict)))\n",
    "\n",
    "# save with text files\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/with_text.txt\",\"w\") as f:\n",
    "    f.write(\"\\n\".join(with_text))\n",
    "    \n",
    "success=0\n",
    "for index,file_name in enumerate(with_text):\n",
    "    file_path=\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted/{}\".\\\n",
    "                format(file_name)\n",
    "    shutil.copy(file_path,\\\n",
    "            \"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/with_text\")\n",
    "    success+=1\n",
    "    print(\"index: {}\".format(index),end=\"\\r\")  \n",
    "print(\"copied {} out of {}\".format(success,\\\n",
    "            len(glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted/*\"))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f49752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 03:29:19,519: INFO Start: Calculating hashes...\n",
      "100%|██████████| 3797/3797 [00:01<00:00, 3586.60it/s]\n",
      "2022-12-12 03:29:20,714: INFO End: Calculating hashes!\n",
      "2022-12-12 03:29:20,716: INFO Start: Evaluating hamming distances for getting duplicates\n",
      "2022-12-12 03:29:20,717: INFO Start: Retrieving duplicates using Cython Brute force algorithm\n",
      "100%|██████████| 3797/3797 [00:00<00:00, 8555.61it/s]\n",
      "2022-12-12 03:29:21,262: INFO End: Retrieving duplicates using Cython Brute force algorithm\n",
      "2022-12-12 03:29:21,262: INFO End: Evaluating hamming distances for getting duplicates\n",
      "2022-12-12 03:29:21,263: INFO Start: Evaluating hamming distances for getting duplicates\n",
      "2022-12-12 03:29:21,264: INFO Start: Retrieving duplicates using Cython Brute force algorithm\n",
      "100%|██████████| 3797/3797 [00:00<00:00, 8553.80it/s]\n",
      "2022-12-12 03:29:21,801: INFO End: Retrieving duplicates using Cython Brute force algorithm\n",
      "2022-12-12 03:29:21,801: INFO End: Evaluating hamming distances for getting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Duplicates: 3797 <class 'dict'>\n",
      "Num. Duplicates (Removal): 3797 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "duplicates=return_duplicates('/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/with_text/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d192922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3797\r"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted.json\",\"r\") as f:\n",
    "    non_corrupted_dict=json.load(f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/ocr_text_only.json\",\"r\") as f:\n",
    "    ocr_text_filtered=json.load(f) \n",
    "    \n",
    "unique_samples=get_unique_samples(duplicates,ocr_text_filtered,\\\n",
    "                                  {key.split(\"/\")[-1]:size for key,size in non_corrupted_dict.items()})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd63bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/duplicate_output.json\",\"w\") as f:\n",
    "    json.dump(duplicates,f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/unique_samples.json\",\"w\") as f:\n",
    "    json.dump(unique_samples,f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e9366356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top priority:  54\n",
      "medium priority:  723\n",
      "low priority:  462\n",
      "least priority:  2554\n"
     ]
    }
   ],
   "source": [
    "# Identifying priority\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted.json\",\"r\") as f:\n",
    "    non_corrupted_dict=json.load(f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/ocr_text_only.json\",\"r\") as f:\n",
    "    ocr_text_filtered=json.load(f) \n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/unique_samples.json\",\"r\") as f:\n",
    "    unique_samples=json.load(f)    \n",
    "    \n",
    "ocr_text_filtered={file:\" \".join(text.split(\"\\n\")) for file,text in ocr_text_filtered.items()}\n",
    "google_insta_text=[(file,ocr_text_filtered[file]) for file in unique_samples if \"reddit_\" not in file]\n",
    "filtered_top_priority=[file for file,text in google_insta_text if any([token.lower() in text.lower().split(\" \")\\\n",
    "                                                                       for token in top_priority_keywords])]\n",
    "print(\"top priority: \",len(filtered_top_priority))\n",
    "\n",
    "useful_tokens=['singapore']+\\\n",
    "[word.lower() for word in list(top_vect.vocabulary_.keys()) if word not in common_eng_wrods and word.isalpha()]\n",
    "\n",
    "google_insta_text=[(file,ocr_text_filtered[file]) for file in unique_samples if \"reddit\" not in file]\n",
    "filtered=[(file,text) for file,text in google_insta_text if any([token.lower() in \\\n",
    "                                                text.lower().split(\" \") for token in useful_tokens])]\n",
    "filtered_medium_priority=[file for file,text in filtered if file not in filtered_top_priority]\n",
    "print(\"medium priority: \",len(filtered_medium_priority))\n",
    "\n",
    "useful_tokens=['singapore']+\\\n",
    "[word.lower() for word in list(vect.vocabulary_.keys()) if word not in common_eng_wrods and word.isalpha()]\n",
    "filtered=[(file,text) for file,text in google_insta_text if any([token.lower() \\\n",
    "                                                    in text.lower().split(\" \") for token in useful_tokens])]\n",
    "\n",
    "filtered_low_priority=[file for file,text in filtered if file not in filtered_top_priority and file not in \\\n",
    "                     filtered_medium_priority ]\n",
    "print(\"low priority: \",len(filtered_low_priority))\n",
    "\n",
    "# least priority - google,insta files not covered above\n",
    "filtered_least_priority=[file for file in unique_samples if file not in [*filtered_top_priority,\\\n",
    "                                *filtered_medium_priority,*filtered_low_priority] and \"reddit\" not in file]\n",
    "print(\"least priority: \",len(filtered_least_priority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a4911261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "with open('/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/filemap.json','r') as f:\n",
    "    file_map=json.load(f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted.json\",\"r\") as f:\n",
    "    non_corrupted_dict=json.load(f)\n",
    "non_corrupted_dict={file.split(\"/\")[-1]:size for file,size in non_corrupted_dict.items()}    \n",
    "    \n",
    "original_names=[file_map[file] for file in unique_samples]\n",
    "texts=[ocr_text_filtered[file] for file in unique_samples]\n",
    "text_lengths=[len(text.split(\" \")) for text in texts]\n",
    "platform=[file.split(\"_\")[0] for file in unique_samples]\n",
    "source=\"instagram\"\n",
    "platform=\"instagram\"\n",
    "keywords=[\",\".join([token for token in ocr_text_filtered[file].split(\" \") if token.lower() in \\\n",
    "                    list(map(lambda x: x.lower(), [*top_priority_keywords,*useful_tokens]))])\\\n",
    "          if file in [*filtered_top_priority,*filtered_medium_priority,*filtered_low_priority] \\\n",
    "              else \"\" for file in unique_samples]\n",
    "image_sizes=[non_corrupted_dict[file] for file in unique_samples]\n",
    "\n",
    "priority={\"file\":unique_samples,\"original file\":original_names,\"text\":texts,\"text length\":text_lengths,\\\n",
    "         \"platform\":platform,\"google search keywords\":source,\"priority keywords\":keywords,\"size\":image_sizes}\n",
    "                  \n",
    "priority=pd.DataFrame(priority)                  \n",
    "priority['priority']=''\n",
    "for i,row in priority.iterrows():\n",
    "    if(row['file'] in filtered_top_priority):\n",
    "        priority.at[i,'priority'] = 'top'\n",
    "    elif(row['file'] in filtered_medium_priority):\n",
    "        priority.at[i,'priority'] = 'medium'\n",
    "    elif(row['file'] in filtered_low_priority):\n",
    "        priority.at[i,'priority'] = 'low'\n",
    "    elif(row['file'] in filtered_least_priority):\n",
    "        priority.at[i,'priority'] = 'least'                  \n",
    "priority.to_csv('/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/priority2.csv',\\\n",
    "                                  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d6679d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12904\r"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/ocr_text_only.json\",\"r\") as f:\n",
    "    ocr_text_filtered=json.load(f) \n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/ocr_text_only.json\",\"r\") as f:\n",
    "    ocr_text_filtered.update(json.load(f))\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/non_corrupted.json\",\"r\") as f:\n",
    "    non_corrupted_dict=json.load(f)\n",
    "with open(\"/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/non_corrupted.json\",\"r\") as f:\n",
    "    non_corrupted_dict.update(json.load(f)) \n",
    "\n",
    "ocr_text_filtered={file:\" \".join(text.split(\"\\n\")) for file,text in ocr_text_filtered.items()}    \n",
    "unique_samples=get_unique_samples(duplicates,ocr_text_filtered,\\\n",
    "                                  {key.split(\"/\")[-1]:size for key,size in non_corrupted_dict.items()})     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
