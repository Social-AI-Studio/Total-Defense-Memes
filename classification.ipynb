{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46572dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob2\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "import re\n",
    "import torchvision\n",
    "resnet = torchvision.models.resnet34(pretrained=True)\n",
    "resnet=nn.Sequential(*list(resnet.children())[:-1])\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "89478a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "with open(\"/mnt/sda/nirmal/rum/code/clustering/sg_memes.json\",\"r\") as f:\n",
    "    data=json.load(f)\n",
    "stance_memes=[item for item in data['matched_memes']]\n",
    "len(stance_memes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1186e97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Social Defence': 28,\n",
       " 'Digital Defence': 26,\n",
       " 'Psychological Defence': 254,\n",
       " 'Economic Defence': 86,\n",
       " 'Civil Defence': 73,\n",
       " 'Military Defence': 511,\n",
       " 'Others': 394}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "pillars=set([pillar for item in stance_memes for pillar in item['pillars']])\n",
    "pillar_wise={pillar:[] for pillar in pillars}\n",
    "for item in stance_memes:\n",
    "    for pillar in item['pillars']:\n",
    "        pillar_wise[pillar].append(item)\n",
    "        break\n",
    "{pillar:len(_) for pillar,_ in pillar_wise.items()}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81b53e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(819, 275, 278)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60-20-20 split\n",
    "train,val,test=[],[],[]\n",
    "for pillar,items in pillar_wise.items():\n",
    "    temp=np.random.choice(items,int(0.60*len(items)),replace=False)\n",
    "    train.extend(temp)\n",
    "    items=[item for item in items if item[\"memeId\"] not in [meme[\"memeId\"] for meme in train]]\n",
    "    temp=np.random.choice(items,int(0.5*len(items)),replace=False)\n",
    "    val.extend(temp)\n",
    "    items=[item for item in items if item[\"memeId\"] not in [meme[\"memeId\"] for meme in val]]\n",
    "    test.extend(items)\n",
    "len(train),len(val),len(test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "493df17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Social Defence': 22,\n",
       "  'Digital Defence': 15,\n",
       "  'Psychological Defence': 152,\n",
       "  'Economic Defence': 71,\n",
       "  'Civil Defence': 47,\n",
       "  'Military Defence': 317,\n",
       "  'Others': 236},\n",
       " {'Social Defence': 9,\n",
       "  'Digital Defence': 7,\n",
       "  'Psychological Defence': 51,\n",
       "  'Economic Defence': 21,\n",
       "  'Civil Defence': 18,\n",
       "  'Military Defence': 106,\n",
       "  'Others': 79},\n",
       " {'Social Defence': 7,\n",
       "  'Digital Defence': 6,\n",
       "  'Psychological Defence': 51,\n",
       "  'Economic Defence': 23,\n",
       "  'Civil Defence': 20,\n",
       "  'Military Defence': 104,\n",
       "  'Others': 79})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train split pillar-wise\n",
    "train_pillar_distr={pillar:0 for pillar in pillars}\n",
    "for item in train:\n",
    "    for pillar in item['pillars']:\n",
    "        train_pillar_distr[pillar]+=1\n",
    "        \n",
    "val_pillar_distr={pillar:0 for pillar in pillars}\n",
    "for item in val:\n",
    "    for pillar in item['pillars']:\n",
    "        val_pillar_distr[pillar]+=1\n",
    "        \n",
    "test_pillar_distr={pillar:0 for pillar in pillars}\n",
    "for item in test:\n",
    "    for pillar in item['pillars']:\n",
    "        test_pillar_distr[pillar]+=1   \n",
    "        \n",
    "train_pillar_distr,val_pillar_distr,test_pillar_distr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4935245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train upsample\n",
    "train_others=[item for item in train if 'Others' in item['pillars']]\n",
    "train_economic=[item for item in train if 'Economic Defence' in item['pillars']]\n",
    "train_social=[item for item in train if 'Social Defence' in item['pillars']]\n",
    "train_digital=[item for item in train if 'Digital Defence' in item['pillars']]\n",
    "train_psychological=[item for item in train if 'Psychological Defence' in item['pillars']]\n",
    "train_civil=[item for item in train if 'Civil Defence' in item['pillars']]\n",
    "train_military=[item for item in train if 'Military Defence' in item['pillars']]\n",
    "\n",
    "train_upsample=[]\n",
    "train_upsample.extend(train_others)\n",
    "train_upsample.extend(np.random.choice(train_others,315-236))\n",
    "\n",
    "train_upsample.extend(train_economic)\n",
    "train_upsample.extend(np.random.choice(train_economic,315-100))\n",
    "\n",
    "train_upsample.extend(train_social)\n",
    "train_upsample.extend(np.random.choice(train_social,315-24)) \n",
    "\n",
    "train_upsample.extend(train_digital)\n",
    "train_upsample.extend(np.random.choice(train_digital,315-17))\n",
    "\n",
    "train_upsample.extend(train_psychological)\n",
    "# train_upsample.extend(np.random.choice(train_psychological,315-300))\n",
    "\n",
    "train_upsample.extend(train_civil)\n",
    "train_upsample.extend(np.random.choice(train_civil,315-51)) \n",
    "\n",
    "train_upsample.extend(train_military)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "57617749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Social Defence': 331,\n",
       " 'Digital Defence': 324,\n",
       " 'Psychological Defence': 329,\n",
       " 'Economic Defence': 319,\n",
       " 'Civil Defence': 360,\n",
       " 'Military Defence': 404,\n",
       " 'Others': 315}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pillar_distr={pillar:0 for pillar in pillars}\n",
    "for item in train_upsample:\n",
    "    for pillar in item['pillars']:\n",
    "        train_pillar_distr[pillar]+=1\n",
    "train_pillar_distr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6ebd7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding image and text info\n",
    "df=pd.read_csv(\"/mnt/sda/nirmal/rum/code/clustering/priority_memes.csv\")\n",
    "data={}\n",
    "image_files=glob2.glob('/mnt/sda/dataset/google_insta_reddit/batch1/postprocessing/with_text/*')\n",
    "image_files.extend(glob2.glob(\"/mnt/sda/dataset/google_insta_reddit/batch2/postprocessing/with_text/*\"))\n",
    "image_file_names=[file.split(\"/\")[-1] for file in image_files]\n",
    "for i,row in df.iterrows():\n",
    "    data[row['filename']]=(row[\"text\"].replace(\"\\n\",\" \"),image_files[image_file_names.index(row['filename'])])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2faf8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding text,image path to data\n",
    "for item in train:\n",
    "    item.update({'text':data[item['filename']][0],'image_path':data[item['filename']][1]})\n",
    "\n",
    "for item in val:\n",
    "    item.update({'text':data[item['filename']][0],'image_path':data[item['filename']][1]})\n",
    "\n",
    "for item in test:\n",
    "    item.update({'text':data[item['filename']][0],'image_path':data[item['filename']][1]})\n",
    "\n",
    "# val=[item.update({'text':data[item['filename']][0],'image_path':data[item['filename']][1]}) for item in val]\n",
    "\n",
    "# test=[item.update({'text':data[item['filename']][0],'image_path':data[item['filename']][1]}) for item in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "01d56f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Social Defence': 0, 'Digital Defence': 1, 'Psychological Defence': 2, 'Economic Defence': 3, 'Civil Defence': 4, 'Military Defence': 5, 'Others': 6} {'Social Defence': [0, 1, 2], 'Digital Defence': [3, 4, 5], 'Psychological Defence': [6, 7, 8], 'Economic Defence': [9, 10, 11], 'Civil Defence': [12, 13, 14], 'Military Defence': [15, 16, 17], 'Others': [18, 19, 20]}\n"
     ]
    }
   ],
   "source": [
    "pillar_labels_dict={pillar:i for i,pillar in enumerate(pillars)}\n",
    "# print(pillar_labels_dict)\n",
    "unique_stances=set([stance for item in stance_memes for _,stance in item['stance']])\n",
    "# # stance_labels_dict={for i,pillar in enumerate(pillars) for indx,stance in enumerate(unique_stances)}\n",
    "unique_stances={stance:i for i,stance in enumerate(unique_stances)}\n",
    "stance_labels_dict={}\n",
    "index=0\n",
    "for pillar,_ in pillar_labels_dict.items():\n",
    "    stance_labels_dict[pillar]=[]\n",
    "    for stance in unique_stances:\n",
    "        stance_labels_dict[pillar].append(index)\n",
    "        index+=1\n",
    "print(pillar_labels_dict,stance_labels_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c7a68a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs=[]\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self,items,embed_typ):\n",
    "        self.items=items\n",
    "        self.embed_typ=embed_typ\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        with open(\"/mnt/sda/nirmal/rum/code/clustering/{}/{}\".\\\n",
    "                  format(self.embed_typ,self.items[index]['filename']),\"rb\") as f:\n",
    "            encoding=pkl.load(f) \n",
    "        \n",
    "        pillar_labels=torch.zeros(7)  \n",
    "        for pillar in self.items[index]['pillars']:\n",
    "            pillar_labels[pillar_labels_dict[pillar]]=1\n",
    "        stance_labels=torch.zeros(21)  \n",
    "        for i,stance in enumerate(self.items[index]['stance']):\n",
    "            if(stance[1]==\"Supportive\"):\n",
    "                stance_labels[stance_labels_dict[stance[0]][0]]=1 \n",
    "            elif(stance[1]==\"Neutral\"):\n",
    "                stance_labels[stance_labels_dict[stance[0]][1]]=1\n",
    "            elif(stance[1]==\"Against\"):\n",
    "                stance_labels[stance_labels_dict[stance[0]][2]]=1      \n",
    "        return encoding,pillar_labels,stance_labels,self.items[index]['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "93af9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pillar_Stance_Classifier(nn.Module):\n",
    "    def __init__(self,embed_typ):\n",
    "        super(Pillar_Stance_Classifier, self).__init__()\n",
    "        if embed_typ==\"VGG\":\n",
    "            self.pillar_cls_layer = torch.nn.Sequential(nn.Linear(25088, 4096),nn.ReLU(),nn.Linear(4096, 512),\\\n",
    "                                                       nn.ReLU(),nn.Linear(512,7))#\n",
    "            self.stance_cls_layer = torch.nn.Sequential(nn.Linear(25088, 4096),nn.ReLU(),nn.Linear(4096, 512),\\\n",
    "                                                       nn.ReLU(),nn.Linear(512,21))\n",
    "        else:\n",
    "            self.pillar_cls_layer = torch.nn.Sequential(nn.Linear(1024, 64),nn.ReLU(),nn.Linear(64,7))\n",
    "            self.stance_cls_layer = torch.nn.Sequential(nn.Linear(1024, 64),nn.ReLU(),nn.Linear(64,21))\n",
    "\n",
    "    def forward(self,inp):\n",
    "        output_pillar=self.pillar_cls_layer(inp)\n",
    "        output_stance=self.stance_cls_layer(inp)\n",
    "        return output_pillar,output_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "15e75f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_typ=\"CLIP\"\n",
    "model=Pillar_Stance_Classifier().to(device)\n",
    "model.train()\n",
    "train_set=ClassifierDataset(train_upsample,embed_typ=embed_typ)\n",
    "val_set=ClassifierDataset(val)\n",
    "train_loader=DataLoader(train_set, batch_size = 64)\n",
    "val_loader = DataLoader(val_set, batch_size = 64)   \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "08acb883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:39.54735004901886 validation loss:5.035736918449402\n",
      "training loss:26.830148458480835 validation loss:3.334825575351715\n",
      "training loss:21.69740927219391 validation loss:3.026538610458374\n",
      "training loss:20.841771066188812 validation loss:2.9722471237182617\n",
      "training loss:20.37072455883026 validation loss:2.9396854043006897\n",
      "training loss:19.923506498336792 validation loss:2.90233713388443\n",
      "training loss:19.467327415943146 validation loss:2.8635897636413574\n",
      "training loss:18.96155270934105 validation loss:2.8170079588890076\n",
      "training loss:18.446918070316315 validation loss:2.7737756967544556\n",
      "training loss:17.878470838069916 validation loss:2.7231356501579285\n",
      "training loss:17.321989744901657 validation loss:2.677002489566803\n",
      "training loss:16.730896025896072 validation loss:2.6267915964126587\n",
      "training loss:16.159560680389404 validation loss:2.5827280580997467\n",
      "training loss:15.569054871797562 validation loss:2.5346655547618866\n",
      "training loss:15.013923168182373 validation loss:2.49394491314888\n",
      "training loss:14.449005097150803 validation loss:2.450942426919937\n",
      "training loss:13.91842320561409 validation loss:2.41276016831398\n",
      "training loss:13.4048533141613 validation loss:2.374281108379364\n",
      "training loss:12.922883585095406 validation loss:2.341840624809265\n",
      "training loss:12.4525675624609 validation loss:2.307611405849457\n",
      "training loss:12.012237310409546 validation loss:2.278210461139679\n",
      "training loss:11.5893012881279 validation loss:2.248252213001251\n",
      "training loss:11.189889311790466 validation loss:2.221180349588394\n",
      "training loss:10.807542935013771 validation loss:2.194943428039551\n",
      "training loss:10.44669857621193 validation loss:2.1719992756843567\n",
      "training loss:10.099432364106178 validation loss:2.147901475429535\n",
      "training loss:9.77580189704895 validation loss:2.1263782680034637\n",
      "training loss:9.45822062343359 validation loss:2.1047954857349396\n",
      "training loss:9.164651028811932 validation loss:2.0849791169166565\n",
      "training loss:8.874035403132439 validation loss:2.064782664179802\n",
      "training loss:8.60813669115305 validation loss:2.046146050095558\n",
      "training loss:8.338800199329853 validation loss:2.028322756290436\n",
      "training loss:8.093938648700714 validation loss:2.0103805661201477\n",
      "training loss:7.84864018112421 validation loss:1.9937224835157394\n",
      "training loss:7.624534510076046 validation loss:1.9777284562587738\n",
      "training loss:7.393405884504318 validation loss:1.961927592754364\n",
      "training loss:7.188016198575497 validation loss:1.9466475993394852\n",
      "training loss:6.97856155782938 validation loss:1.9319448173046112\n",
      "training loss:6.783789083361626 validation loss:1.917455792427063\n",
      "training loss:6.5937970243394375 validation loss:1.9049994349479675\n",
      "training loss:6.411103241145611 validation loss:1.8917359113693237\n",
      "training loss:6.236521415412426 validation loss:1.8796416521072388\n",
      "training loss:6.06974408775568 validation loss:1.8672270774841309\n",
      "training loss:5.9053025133907795 validation loss:1.857319414615631\n",
      "training loss:5.7530827075243 validation loss:1.8454500138759613\n",
      "training loss:5.600393995642662 validation loss:1.836652547121048\n",
      "training loss:5.460483528673649 validation loss:1.8259900212287903\n",
      "training loss:5.318720933049917 validation loss:1.8175833374261856\n",
      "training loss:5.189081329852343 validation loss:1.808410882949829\n",
      "training loss:5.058435693383217 validation loss:1.8014618754386902\n",
      "training loss:4.937752664089203 validation loss:1.7929219007492065\n",
      "training loss:4.8168384321033955 validation loss:1.7873066365718842\n",
      "training loss:4.706265561282635 validation loss:1.7796760946512222\n",
      "training loss:4.592914156615734 validation loss:1.7750051021575928\n",
      "training loss:4.490392168983817 validation loss:1.7683453559875488\n",
      "training loss:4.385107904672623 validation loss:1.764376923441887\n",
      "training loss:4.289974229410291 validation loss:1.7585485577583313\n",
      "training loss:4.191896492615342 validation loss:1.755470186471939\n",
      "training loss:4.102472277358174 validation loss:1.7505842596292496\n",
      "training loss:4.011478887870908 validation loss:1.7479970753192902\n",
      "training loss:3.9289441015571356 validation loss:1.7438441514968872\n",
      "training loss:3.8425759989768267 validation loss:1.741988092660904\n",
      "training loss:3.7652068212628365 validation loss:1.7383766174316406\n",
      "training loss:3.6845732908695936 validation loss:1.7372408509254456\n",
      "training loss:3.6111987736076117 validation loss:1.7346074730157852\n",
      "training loss:3.5371620394289494 validation loss:1.7330979108810425\n",
      "training loss:3.4659081734716892 validation loss:1.7316690683364868\n",
      "training loss:3.3966542091220617 validation loss:1.730583906173706\n",
      "training loss:3.3291535545140505 validation loss:1.7298224866390228\n",
      "training loss:3.265010442584753 validation loss:1.7288252413272858\n",
      "training loss:3.199290918186307 validation loss:1.7287145256996155\n",
      "training loss:3.1386231193318963 validation loss:1.7282316237688065\n",
      "training loss:3.077880627475679 validation loss:1.7282866686582565\n",
      "training loss:3.0189675595611334 validation loss:1.7284435331821442\n",
      "training loss:2.9613083861768246 validation loss:1.7284899055957794\n",
      "training loss:2.9052568105980754 validation loss:1.7291535139083862\n",
      "training loss:2.8507477939128876 validation loss:1.7297264635562897\n",
      "training loss:2.797135330736637 validation loss:1.7306404411792755\n",
      "training loss:2.745716986246407 validation loss:1.731713593006134\n",
      "training loss:2.694211468100548 validation loss:1.732750803232193\n",
      "training loss:2.6445127511397004 validation loss:1.7343806624412537\n",
      "training loss:2.5963990427553654 validation loss:1.7356234788894653\n",
      "training loss:2.5483431350439787 validation loss:1.7374937683343887\n",
      "training loss:2.5021167509257793 validation loss:1.738991305232048\n",
      "training loss:2.4564860528334975 validation loss:1.740922600030899\n",
      "training loss:2.4116891836747527 validation loss:1.7429313510656357\n",
      "training loss:2.368876000866294 validation loss:1.7448034137487411\n",
      "training loss:2.324459109455347 validation loss:1.7472105622291565\n",
      "training loss:2.2845751717686653 validation loss:1.7494048476219177\n",
      "training loss:2.2416187170892954 validation loss:1.7520171701908112\n",
      "training loss:2.2024709871038795 validation loss:1.7545190751552582\n",
      "training loss:2.16240724734962 validation loss:1.7571813315153122\n",
      "training loss:2.1237757122144103 validation loss:1.7600145041942596\n",
      "training loss:2.085582096129656 validation loss:1.7628368139266968\n",
      "training loss:2.048535745590925 validation loss:1.7659701853990555\n",
      "training loss:2.0111331627704203 validation loss:1.7691745460033417\n",
      "training loss:1.976163299754262 validation loss:1.7722632586956024\n",
      "training loss:1.939778732135892 validation loss:1.7756173312664032\n",
      "training loss:1.9055754952132702 validation loss:1.7790978699922562\n",
      "training loss:1.8712285961955786 validation loss:1.7824117243289948\n",
      "training loss:1.8378798910416663 validation loss:1.7861599922180176\n",
      "training loss:1.8040637094527483 validation loss:1.7898417711257935\n",
      "training loss:1.77283521508798 validation loss:1.7933438420295715\n",
      "training loss:1.7391725946217775 validation loss:1.7974608540534973\n",
      "training loss:1.7096435762941837 validation loss:1.801067665219307\n",
      "training loss:1.6770254727452993 validation loss:1.8052519261837006\n",
      "training loss:1.6479739216156304 validation loss:1.8090556114912033\n",
      "training loss:1.6167835416272283 validation loss:1.813183307647705\n",
      "training loss:1.5885828519240022 validation loss:1.8175084590911865\n",
      "training loss:1.5587545996531844 validation loss:1.8215495347976685\n",
      "training loss:1.5306411748751998 validation loss:1.8259934931993484\n",
      "training loss:1.5020825108513236 validation loss:1.8304884731769562\n",
      "training loss:1.4750389787368476 validation loss:1.835051417350769\n",
      "training loss:1.4475035751238465 validation loss:1.8393931090831757\n",
      "training loss:1.4208725173957646 validation loss:1.8444222807884216\n",
      "training loss:1.3946828553453088 validation loss:1.8485788702964783\n",
      "training loss:1.368559043854475 validation loss:1.8539350032806396\n",
      "training loss:1.343225818593055 validation loss:1.858277827501297\n",
      "training loss:1.3178624589927495 validation loss:1.8633792102336884\n",
      "training loss:1.2935149888508022 validation loss:1.8679428696632385\n",
      "training loss:1.2686303965747356 validation loss:1.8733141720294952\n",
      "training loss:1.245421895524487 validation loss:1.877746269106865\n",
      "training loss:1.2207688703201711 validation loss:1.8836179971694946\n",
      "training loss:1.1986139179207385 validation loss:1.888003408908844\n",
      "training loss:1.174761717673391 validation loss:1.894028753042221\n",
      "training loss:1.1531417870428413 validation loss:1.8987112045288086\n",
      "training loss:1.1302704440895468 validation loss:1.9046432971954346\n",
      "training loss:1.109038540162146 validation loss:1.909591183066368\n",
      "training loss:1.0868741411250085 validation loss:1.9158166497945786\n",
      "training loss:1.0665463777258992 validation loss:1.92082779109478\n",
      "training loss:1.0451135074254125 validation loss:1.92697674036026\n",
      "training loss:1.0253495099022985 validation loss:1.9319919645786285\n",
      "training loss:1.0042625796049833 validation loss:1.9387576431035995\n",
      "training loss:0.9858897544909269 validation loss:1.9435796737670898\n",
      "training loss:0.9649895371403545 validation loss:1.9504403471946716\n",
      "training loss:0.9474817411974072 validation loss:1.9551964104175568\n",
      "training loss:0.9270657887682319 validation loss:1.9621624648571014\n",
      "training loss:0.9097097471822053 validation loss:1.9673805981874466\n",
      "training loss:0.8908670849632472 validation loss:1.9737747609615326\n",
      "training loss:0.8731289193965495 validation loss:1.9798342883586884\n",
      "training loss:0.8553188301157206 validation loss:1.9867863059043884\n",
      "training loss:0.8389383205212653 validation loss:1.991789311170578\n",
      "training loss:0.820784839335829 validation loss:1.9991736263036728\n",
      "training loss:0.8050753478892148 validation loss:2.004564866423607\n",
      "training loss:0.787897418718785 validation loss:2.0119569301605225\n",
      "training loss:0.7723195992875844 validation loss:2.017600566148758\n",
      "training loss:0.7560109179466963 validation loss:2.0246423482894897\n",
      "training loss:0.7406834966968745 validation loss:2.0310704112052917\n",
      "training loss:0.7255497148726135 validation loss:2.037393182516098\n",
      "training loss:0.7100184538867325 validation loss:2.044338285923004\n",
      "training loss:0.6957045523449779 validation loss:2.0510376691818237\n",
      "training loss:0.6809880877844989 validation loss:2.057670444250107\n",
      "training loss:0.6670979429036379 validation loss:2.0644984543323517\n",
      "training loss:0.6528300871141255 validation loss:2.071399062871933\n",
      "training loss:0.6391848865896463 validation loss:2.0781681537628174\n",
      "training loss:0.6257637687958777 validation loss:2.0850492864847183\n",
      "training loss:0.6124981407774612 validation loss:2.0923235416412354\n",
      "training loss:0.5996461323229596 validation loss:2.0989473909139633\n",
      "training loss:0.5869994601234794 validation loss:2.105978414416313\n",
      "training loss:0.5739585837582126 validation loss:2.11347895860672\n",
      "training loss:0.5625519579043612 validation loss:2.120336502790451\n",
      "training loss:0.5497406269423664 validation loss:2.127901166677475\n",
      "training loss:0.5387798198498785 validation loss:2.1343817561864853\n",
      "training loss:0.5260986180510372 validation loss:2.142416775226593\n",
      "training loss:0.515831513912417 validation loss:2.149686321616173\n",
      "training loss:0.5042382925748825 validation loss:2.1566777378320694\n",
      "training loss:0.4936726811574772 validation loss:2.164644405245781\n",
      "training loss:0.4829432686092332 validation loss:2.1716631650924683\n",
      "training loss:0.4723662647884339 validation loss:2.179484874010086\n",
      "training loss:0.46220357983838767 validation loss:2.1870301365852356\n",
      "training loss:0.4521749570267275 validation loss:2.1947552263736725\n",
      "training loss:0.4426911489572376 validation loss:2.2016881108283997\n",
      "training loss:0.43254878907464445 validation loss:2.2101043462753296\n",
      "training loss:0.42382256453856826 validation loss:2.216988444328308\n",
      "training loss:0.4140837723389268 validation loss:2.2250940054655075\n",
      "training loss:0.4052290190011263 validation loss:2.2324270457029343\n",
      "training loss:0.3966958582168445 validation loss:2.2402751743793488\n",
      "training loss:0.3876732367789373 validation loss:2.2479001730680466\n",
      "training loss:0.37929846241604537 validation loss:2.256002977490425\n",
      "training loss:0.3712218654109165 validation loss:2.263252541422844\n",
      "training loss:0.3629638197598979 validation loss:2.2715694308280945\n",
      "training loss:0.3550827798899263 validation loss:2.279163360595703\n",
      "training loss:0.3472815314307809 validation loss:2.287066251039505\n",
      "training loss:0.3397132783429697 validation loss:2.2944466173648834\n",
      "training loss:0.33237572212237865 validation loss:2.3027101010084152\n",
      "training loss:0.3248560306383297 validation loss:2.3105007112026215\n",
      "training loss:0.31820055103162304 validation loss:2.3182225078344345\n",
      "training loss:0.3108033826574683 validation loss:2.3265458196401596\n",
      "training loss:0.3042300959932618 validation loss:2.334156036376953\n",
      "training loss:0.29766218696022406 validation loss:2.3421302437782288\n",
      "training loss:0.29103141190716997 validation loss:2.3497685194015503\n",
      "training loss:0.28454028646228835 validation loss:2.3584505021572113\n",
      "training loss:0.2786632950301282 validation loss:2.3657388985157013\n",
      "training loss:0.27225173747865483 validation loss:2.37424099445343\n",
      "training loss:0.2666453709243797 validation loss:2.3819405138492584\n",
      "training loss:0.26061140198726207 validation loss:2.3900631070137024\n",
      "training loss:0.2549464651965536 validation loss:2.3981235027313232\n",
      "training loss:0.24946294294204563 validation loss:2.4057267159223557\n",
      "training loss:0.24394065607339144 validation loss:2.414568245410919\n",
      "training loss:0.23873717500828207 validation loss:2.421997457742691\n",
      "training loss:0.23355037561850622 validation loss:2.430635094642639\n",
      "training loss:0.22861918120179325 validation loss:2.4378072023391724\n",
      "training loss:0.2233353330520913 validation loss:2.446564644575119\n",
      "training loss:0.218722588440869 validation loss:2.4548154771327972\n",
      "training loss:0.2140143394935876 validation loss:2.4623758643865585\n",
      "training loss:0.20920944237150252 validation loss:2.470740243792534\n",
      "training loss:0.20496701129013672 validation loss:2.4787096679210663\n",
      "training loss:0.20039750868454576 validation loss:2.4869900047779083\n",
      "training loss:0.1960904833395034 validation loss:2.4958071410655975\n",
      "training loss:0.19203125493368134 validation loss:2.502420336008072\n",
      "training loss:0.18769155844347551 validation loss:2.511877179145813\n",
      "training loss:0.18383341433946043 validation loss:2.519248902797699\n",
      "training loss:0.1797325150691904 validation loss:2.528308391571045\n",
      "training loss:0.17617578199133277 validation loss:2.53512105345726\n",
      "training loss:0.17211209458764642 validation loss:2.5444521605968475\n",
      "training loss:0.16861470171716064 validation loss:2.552041321992874\n",
      "training loss:0.16498478967696428 validation loss:2.560495674610138\n",
      "training loss:0.16138938412768766 validation loss:2.568374454975128\n",
      "training loss:0.15811643298366107 validation loss:2.577171742916107\n",
      "training loss:0.1546659370069392 validation loss:2.5846391022205353\n",
      "training loss:0.15146893629571423 validation loss:2.593406856060028\n",
      "training loss:0.148266258998774 validation loss:2.6007347106933594\n",
      "training loss:0.14496598753612489 validation loss:2.6099606454372406\n",
      "training loss:0.14227193023543805 validation loss:2.616882771253586\n",
      "training loss:0.13878036971436813 validation loss:2.6270674765110016\n",
      "training loss:0.13643125412636437 validation loss:2.6328272223472595\n",
      "training loss:0.13296489254571497 validation loss:2.643651098012924\n",
      "training loss:0.13082302987459116 validation loss:2.648975133895874\n",
      "training loss:0.12749452801654115 validation loss:2.659525156021118\n",
      "training loss:0.12531549273990095 validation loss:2.6659659147262573\n",
      "training loss:0.12223867961438373 validation loss:2.676071584224701\n",
      "training loss:0.12019848541240208 validation loss:2.682247221469879\n",
      "training loss:0.11726128251757473 validation loss:2.6924279928207397\n",
      "training loss:0.1151323638332542 validation loss:2.699107825756073\n",
      "training loss:0.11253415216924623 validation loss:2.7086431086063385\n",
      "training loss:0.11026214188314043 validation loss:2.7158666849136353\n",
      "training loss:0.10807718045543879 validation loss:2.7246327996253967\n",
      "training loss:0.10564505608635955 validation loss:2.732869476079941\n",
      "training loss:0.10368085093796253 validation loss:2.7407279014587402\n",
      "training loss:0.10143761170911603 validation loss:2.748701959848404\n",
      "training loss:0.0993931658740621 validation loss:2.75772887468338\n",
      "training loss:0.09728665728471242 validation loss:2.7654453217983246\n",
      "training loss:0.0953715267824009 validation loss:2.7735973596572876\n",
      "training loss:0.09338990991818719 validation loss:2.7816645205020905\n",
      "training loss:0.09147538442630321 validation loss:2.7901979088783264\n",
      "training loss:0.08957793875015341 validation loss:2.7981957495212555\n",
      "training loss:0.08786016996600665 validation loss:2.806095540523529\n",
      "training loss:0.08587792865000665 validation loss:2.815600663423538\n",
      "training loss:0.08433615320245735 validation loss:2.822319895029068\n",
      "training loss:0.08248470159014687 validation loss:2.8313634395599365\n",
      "training loss:0.0809137616015505 validation loss:2.8391700983047485\n",
      "training loss:0.07919098518323153 validation loss:2.847837597131729\n",
      "training loss:0.07767719324328937 validation loss:2.855148285627365\n",
      "training loss:0.07600355730392039 validation loss:2.864282965660095\n",
      "training loss:0.07457793562207371 validation loss:2.871770679950714\n",
      "training loss:0.07296261984447483 validation loss:2.8805662095546722\n",
      "training loss:0.07157102522614878 validation loss:2.8882344365119934\n",
      "training loss:0.07015673800196964 validation loss:2.896283447742462\n",
      "training loss:0.06870288150093984 validation loss:2.9046751260757446\n",
      "training loss:0.06730654278362636 validation loss:2.913007438182831\n",
      "training loss:0.06602876870601904 validation loss:2.920447051525116\n",
      "training loss:0.0646476124675246 validation loss:2.928951233625412\n",
      "training loss:0.0633978328842204 validation loss:2.93709933757782\n",
      "training loss:0.062116225803038105 validation loss:2.945463329553604\n",
      "training loss:0.06089685647748411 validation loss:2.9533589482307434\n",
      "training loss:0.05963939634966664 validation loss:2.9613808393478394\n",
      "training loss:0.0584709252580069 validation loss:2.9700640439987183\n",
      "training loss:0.05734042492986191 validation loss:2.9775184094905853\n",
      "training loss:0.05618807495920919 validation loss:2.985964000225067\n",
      "training loss:0.05501368087425362 validation loss:2.9947119057178497\n",
      "training loss:0.05407451660721563 validation loss:3.0019717812538147\n",
      "training loss:0.05286810063989833 validation loss:3.0101285874843597\n",
      "training loss:0.05190956975275185 validation loss:3.01846119761467\n",
      "training loss:0.05081809814146254 validation loss:3.0258940756320953\n",
      "training loss:0.04988446188508533 validation loss:3.0349007546901703\n",
      "training loss:0.048854804059374146 validation loss:3.0429687798023224\n",
      "training loss:0.04795402352465317 validation loss:3.0503495633602142\n",
      "training loss:0.046897418636945076 validation loss:3.0595068633556366\n",
      "training loss:0.04614392494841013 validation loss:3.0667372047901154\n",
      "training loss:0.045110436563845724 validation loss:3.075398236513138\n",
      "training loss:0.04430926140048541 validation loss:3.0830423831939697\n",
      "training loss:0.043393174913944677 validation loss:3.091296434402466\n",
      "training loss:0.042570640245685354 validation loss:3.0995675027370453\n",
      "training loss:0.041737777093658224 validation loss:3.1073819994926453\n",
      "training loss:0.040952277078758925 validation loss:3.1157390475273132\n",
      "training loss:0.04008597647771239 validation loss:3.123894155025482\n",
      "training loss:0.039392252583638765 validation loss:3.131955921649933\n",
      "training loss:0.03859178633138072 validation loss:3.1393337845802307\n",
      "training loss:0.03785710742522497 validation loss:3.1482655107975006\n",
      "training loss:0.03711219859542325 validation loss:3.1560201942920685\n",
      "training loss:0.03636971488595009 validation loss:3.1646026968955994\n",
      "training loss:0.0357489629968768 validation loss:3.171726882457733\n",
      "training loss:0.0349578464665683 validation loss:3.1810726523399353\n",
      "training loss:0.03439495181373786 validation loss:3.187612295150757\n",
      "training loss:0.03364366177993361 validation loss:3.196776032447815\n",
      "training loss:0.03305643245403189 validation loss:3.2041762471199036\n",
      "training loss:0.032367575433454476 validation loss:3.21305975317955\n",
      "training loss:0.03180611885909457 validation loss:3.220161020755768\n",
      "training loss:0.031146919580351096 validation loss:3.228682667016983\n",
      "training loss:0.03057462949072942 validation loss:3.236295521259308\n",
      "training loss:0.029952321339806076 validation loss:3.245870292186737\n",
      "training loss:0.029476252530002967 validation loss:3.2517127990722656\n",
      "training loss:0.028793208482966293 validation loss:3.2617934942245483\n",
      "training loss:0.028357255803712178 validation loss:3.26808699965477\n",
      "training loss:0.02769102817546809 validation loss:3.2773537933826447\n",
      "training loss:0.027287664881441742 validation loss:3.2846372723579407\n",
      "training loss:0.026700148278905544 validation loss:3.2929317951202393\n",
      "training loss:0.026213693970930763 validation loss:3.3010273575782776\n",
      "training loss:0.02570065089093987 validation loss:3.3088908195495605\n",
      "training loss:0.02522635085915681 validation loss:3.3170683681964874\n",
      "training loss:0.02475976213463582 validation loss:3.324677884578705\n",
      "training loss:0.024258222605567425 validation loss:3.3336859345436096\n",
      "training loss:0.023837529530283064 validation loss:3.341018706560135\n",
      "training loss:0.023369100417767186 validation loss:3.349287897348404\n",
      "training loss:0.02293174428632483 validation loss:3.3575945496559143\n",
      "training loss:0.022513366435305215 validation loss:3.3644871413707733\n",
      "training loss:0.022042806471290532 validation loss:3.3734765350818634\n",
      "training loss:0.021672056085662916 validation loss:3.38119900226593\n",
      "training loss:0.021225423683063127 validation loss:3.389448821544647\n",
      "training loss:0.020875076239462942 validation loss:3.3970484137535095\n",
      "training loss:0.020428104922757484 validation loss:3.405914396047592\n",
      "training loss:0.020115016035560984 validation loss:3.4126206636428833\n",
      "training loss:0.0196620835486101 validation loss:3.42189759016037\n",
      "training loss:0.01937640791584272 validation loss:3.428616225719452\n",
      "training loss:0.018926609140180517 validation loss:3.437890410423279\n",
      "training loss:0.018648409299203195 validation loss:3.44498673081398\n",
      "training loss:0.018233837974548806 validation loss:3.453823059797287\n",
      "training loss:0.017951218353118747 validation loss:3.4608961939811707\n",
      "training loss:0.017559566389536485 validation loss:3.469930648803711\n",
      "training loss:0.01729388634339557 validation loss:3.476559817790985\n",
      "training loss:0.016912944251089357 validation loss:3.4858024418354034\n",
      "training loss:0.016652982729283394 validation loss:3.492663264274597\n",
      "training loss:0.016292141903250013 validation loss:3.50149729847908\n",
      "training loss:0.01603566517951549 validation loss:3.5092230439186096\n",
      "training loss:0.01569301811468904 validation loss:3.5173248052597046\n",
      "training loss:0.015451367882633349 validation loss:3.5246083736419678\n",
      "training loss:0.015097868057637243 validation loss:3.5343200862407684\n",
      "training loss:0.014896364311425714 validation loss:3.539881944656372\n",
      "training loss:0.014540677584591322 validation loss:3.550341099500656\n",
      "training loss:0.014360406319610775 validation loss:3.555832266807556\n",
      "training loss:0.014017666224390268 validation loss:3.56511527299881\n",
      "training loss:0.013804547990730498 validation loss:3.5729907751083374\n",
      "training loss:0.013517650295398198 validation loss:3.580643445253372\n",
      "training loss:0.013301945102284662 validation loss:3.5884646475315094\n",
      "training loss:0.013012098341278033 validation loss:3.597417801618576\n",
      "training loss:0.012824757690395927 validation loss:3.6039330661296844\n",
      "training loss:0.012535900827060686 validation loss:3.61385178565979\n",
      "training loss:0.01235703280690359 validation loss:3.6197453141212463\n",
      "training loss:0.0120775659706851 validation loss:3.6296503841876984\n",
      "training loss:0.011899941644514911 validation loss:3.6363088190555573\n",
      "training loss:0.01165864386348403 validation loss:3.644786834716797\n",
      "training loss:0.01144141217082506 validation loss:3.652556926012039\n",
      "training loss:0.011247392245422816 validation loss:3.6604374647140503\n",
      "training loss:0.011027942608052399 validation loss:3.668007642030716\n",
      "training loss:0.010826734385773307 validation loss:3.6768367886543274\n",
      "training loss:0.01063457074269536 validation loss:3.684570163488388\n",
      "training loss:0.010451013222336769 validation loss:3.691600203514099\n",
      "training loss:0.010230072246486088 validation loss:3.7008039355278015\n",
      "training loss:0.010074743451696122 validation loss:3.7077214419841766\n",
      "training loss:0.009870476726064226 validation loss:3.7159023880958557\n",
      "training loss:0.009691572493466083 validation loss:3.723968416452408\n",
      "training loss:0.009511788226518547 validation loss:3.731648027896881\n",
      "training loss:0.009349611373181688 validation loss:3.7398176789283752\n",
      "training loss:0.009161530979326926 validation loss:3.748258113861084\n",
      "training loss:0.00902275604676106 validation loss:3.755557060241699\n",
      "training loss:0.008834568991005654 validation loss:3.7635752856731415\n",
      "training loss:0.00868538778013317 validation loss:3.7717003524303436\n",
      "training loss:0.00852458157169167 validation loss:3.7790430188179016\n",
      "training loss:0.008358722985576605 validation loss:3.788342148065567\n",
      "training loss:0.008230486078900867 validation loss:3.794702112674713\n",
      "training loss:0.008057093999013887 validation loss:3.8036085963249207\n",
      "training loss:0.007927105183625827 validation loss:3.810824692249298\n",
      "training loss:0.007776438622386195 validation loss:3.8194978833198547\n",
      "training loss:0.007636863078005263 validation loss:3.8271851539611816\n",
      "training loss:0.007497101256376482 validation loss:3.8347814083099365\n",
      "training loss:0.007362148344327579 validation loss:3.842943847179413\n",
      "training loss:0.00722913446406892 validation loss:3.8501132130622864\n",
      "training loss:0.007097763284036773 validation loss:3.858605146408081\n",
      "training loss:0.006964437845454086 validation loss:3.865985721349716\n",
      "training loss:0.006843749339168426 validation loss:3.8751946687698364\n",
      "training loss:0.006722283773342497 validation loss:3.8814065158367157\n",
      "training loss:0.006592797315533971 validation loss:3.891105532646179\n",
      "training loss:0.00648966351764102 validation loss:3.8971219062805176\n",
      "training loss:0.006354858929626062 validation loss:3.9070029258728027\n",
      "training loss:0.006256631040741922 validation loss:3.912882387638092\n",
      "training loss:0.0061240695576998405 validation loss:3.9229376912117004\n",
      "training loss:0.006031368393450975 validation loss:3.928530305624008\n",
      "training loss:0.00590916626060789 validation loss:3.9389780163764954\n",
      "training loss:0.005816943967147381 validation loss:3.9443533420562744\n",
      "training loss:0.005701506719560712 validation loss:3.954225867986679\n",
      "training loss:0.005608027382550063 validation loss:3.9603307843208313\n",
      "training loss:0.00549347958076396 validation loss:3.970322072505951\n",
      "training loss:0.005414219278463861 validation loss:3.976290851831436\n",
      "training loss:0.0052982840134063736 validation loss:3.9854907989501953\n",
      "training loss:0.005217422260102467 validation loss:3.9921214282512665\n",
      "training loss:0.0051103343757858966 validation loss:4.001441478729248\n",
      "training loss:0.005031689515817561 validation loss:4.007994711399078\n",
      "training loss:0.0049311544080410386 validation loss:4.017120391130447\n",
      "training loss:0.004849275608648895 validation loss:4.02376863360405\n",
      "training loss:0.004755176247272175 validation loss:4.033108949661255\n",
      "training loss:0.004681593611167045 validation loss:4.039222300052643\n",
      "training loss:0.004581948758641374 validation loss:4.048884987831116\n",
      "training loss:0.004519571284617996 validation loss:4.054832935333252\n",
      "training loss:0.004418271899339743 validation loss:4.064721494913101\n",
      "training loss:0.004360633847682038 validation loss:4.070924341678619\n",
      "training loss:0.004261871821654495 validation loss:4.080326557159424\n",
      "training loss:0.004205948778690072 validation loss:4.0863208174705505\n",
      "training loss:0.004115814814213081 validation loss:4.095610737800598\n",
      "training loss:0.004048730742397311 validation loss:4.101928770542145\n",
      "training loss:0.003968105847889092 validation loss:4.111822068691254\n",
      "training loss:0.003910477957106195 validation loss:4.117864549160004\n",
      "training loss:0.0038292918861770886 validation loss:4.127835750579834\n",
      "training loss:0.003770566980165313 validation loss:4.133833318948746\n",
      "training loss:0.003695944090395642 validation loss:4.142982214689255\n",
      "training loss:0.0036356548698677216 validation loss:4.149885773658752\n",
      "training loss:0.00356475877288176 validation loss:4.158687055110931\n",
      "training loss:0.0035103453092233394 validation loss:4.1647337675094604\n",
      "training loss:0.0034323468262300594 validation loss:4.175542116165161\n",
      "training loss:0.0033930784729818697 validation loss:4.180135488510132\n",
      "training loss:0.0033148333295685006 validation loss:4.190467000007629\n",
      "training loss:0.0032686928834664286 validation loss:4.1959245800971985\n",
      "training loss:0.0031994153760024346 validation loss:4.206242680549622\n",
      "training loss:0.0031541302414552774 validation loss:4.211354315280914\n",
      "training loss:0.0030847653270029696 validation loss:4.222626328468323\n",
      "training loss:0.0030441701082963846 validation loss:4.22679877281189\n",
      "training loss:0.0029778644720863667 validation loss:4.2379539012908936\n",
      "training loss:0.0029353891522987396 validation loss:4.2431860268116\n",
      "training loss:0.002872108131668938 validation loss:4.253296375274658\n",
      "training loss:0.002831703769516025 validation loss:4.259043604135513\n",
      "training loss:0.0027742876054617227 validation loss:4.268672525882721\n",
      "training loss:0.0027320665094521246 validation loss:4.274298697710037\n",
      "training loss:0.0026765768861878314 validation loss:4.284497141838074\n",
      "training loss:0.00263713299864321 validation loss:4.290236264467239\n",
      "training loss:0.002583643026810023 validation loss:4.299642086029053\n",
      "training loss:0.0025420023057449725 validation loss:4.306457281112671\n",
      "training loss:0.002492906656698324 validation loss:4.315505117177963\n",
      "training loss:0.0024544523794247652 validation loss:4.322291135787964\n",
      "training loss:0.0024077399511952535 validation loss:4.330990821123123\n",
      "training loss:0.002367193989812222 validation loss:4.3382983803749084\n",
      "training loss:0.002322762615222018 validation loss:4.346044361591339\n",
      "training loss:0.0022847211939733825 validation loss:4.353806555271149\n",
      "training loss:0.0022434904449255555 validation loss:4.361461400985718\n",
      "training loss:0.0022017281316948356 validation loss:4.369293719530106\n",
      "training loss:0.002163879443287442 validation loss:4.3778268694877625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-f49fe302feb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_e\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpillar_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstance_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#         input_ids=embedding['input_ids'].to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-3f12dac343ba>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m         with open(\"/mnt/sda/nirmal/rum/code/clustering/clip_embeddings/{}\".\\\n\u001b[1;32m     19\u001b[0m                   format(self.items[index]['filename']),\"rb\") as f:\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_embeds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_embeds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mdeserialized_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mtypename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_losses=[]\n",
    "for _e in range(1000):\n",
    "    train_loss=0\n",
    "    for t, (embedding, pillar_labels,stance_labels) in enumerate(train_loader):\n",
    "        embedding=embedding.to(device)\n",
    "        \n",
    "        pillar_labels =pillar_labels.to(device) \n",
    "        stance_labels =stance_labels.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pillar_logits,stance_logits=model(embedding)\n",
    "        pillar_loss = criterion(pillar_logits.squeeze(1), pillar_labels.float())\n",
    "        stance_loss = criterion(stance_logits.squeeze(1), stance_labels.float())\n",
    "        \n",
    "        loss=pillar_loss+stance_loss\n",
    "        train_loss+=loss.data.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss= np.mean(train_loss)\n",
    "    val_loss=0\n",
    "    for t, (embedding,pillar_labels,stance_labels) in enumerate(val_loader):\n",
    "        embedding=embedding.to(device)\n",
    "        pillar_labels =pillar_labels.to(device) \n",
    "        stance_labels =stance_labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pillar_logits,stance_logits=model(embedding)\n",
    "        pillar_loss = criterion(pillar_logits.squeeze(1), pillar_labels.float())\n",
    "        stance_loss = criterion(stance_logits.squeeze(1), stance_labels.float())\n",
    "        val_loss+=(pillar_loss+stance_loss).data.item()\n",
    "    val_loss= np.mean(val_loss)   \n",
    "    if(len(val_losses)>0 and val_loss<min(val_losses)):\n",
    "          torch.save(model.state_dict(), '{}.pt'.format(embed_typ))  \n",
    "    val_losses.append(val_loss)      \n",
    "    print('training loss:{} validation loss:{}'.format(train_loss,val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "77c01c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Pillar_Stance_Classifier().to(device)\n",
    "test_set=ClassifierDataset(test)\n",
    "test_loader=DataLoader(test_set, batch_size = 1,shuffle=False)\n",
    "model.load_state_dict(torch.load(\"model_clip.pt\"))\n",
    "\n",
    "model.eval()\n",
    "preds=[]\n",
    "pillar_outputs=[]\n",
    "stance_outputs=[]\n",
    "pillar_labels_gt=[]\n",
    "stance_labels_gt=[]\n",
    "stance_output_proba=[]\n",
    "filenames=[]\n",
    "with torch.no_grad():\n",
    "    for t, (embedding,pillar_labels,stance_labels,filename) in enumerate(test_loader):\n",
    "        filenames.append(filename)\n",
    "        embedding=embedding.to(device)\n",
    "        pillar_labels =pillar_labels.to(device) \n",
    "        stance_labels =stance_labels.to(device)  \n",
    "        pillar_logits,stance_logits=model(embedding)\n",
    "        pillar_output,stance_output=torch.sigmoid(pillar_logits),torch.sigmoid(stance_logits)\n",
    "        temp=[0 if item<0.5 else 1 for item in pillar_output.view(-1).tolist()]\n",
    "\n",
    "        pillar_outputs.append(temp) \n",
    "        stance_outputs.append([0 if item<0.5 else 1 for item in stance_output.view(-1).tolist()])\n",
    "        stance_output_proba.append(stance_output.view(-1).tolist())\n",
    "        pillar_labels_gt.append(pillar_labels.view(-1).tolist())\n",
    "        stance_labels_gt.append(stance_labels.view(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d5bfe6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.00      0.00      0.00        32\\n           1       0.80      0.27      0.40        15\\n           2       0.68      0.92      0.78       106\\n           3       0.45      0.51      0.48        39\\n\\n    accuracy                           0.63       192\\n   macro avg       0.48      0.42      0.42       192\\nweighted avg       0.53      0.63      0.56       192\\n'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt=[[item[n*3:(n+1)*3] for n in range(7)] for item in stance_labels_gt]\n",
    "stance_outputs=[[item[n*3:(n+1)*3] for n in range(7)] for item in stance_output_proba]\n",
    "\n",
    "pred_filtered=[]\n",
    "gt_filtered=[]\n",
    "for i,item in enumerate(pillar_outputs):\n",
    "    temp=np.array(item)\n",
    "    indices=[it for item in np.where(temp==1) for it in item]\n",
    "    for indx in indices:\n",
    "#         pred_filtered.append(pred[i][indx])\n",
    "        arg=np.argmax(stance_outputs[i][indx])\n",
    "        \n",
    "        if(arg==0):\n",
    "            pred_filtered.append(1)\n",
    "        elif(arg==1):\n",
    "            pred_filtered.append(2)\n",
    "        elif(arg==2):\n",
    "            pred_filtered.append(3)\n",
    "        if(gt[i][indx]==[0,0,0]):\n",
    "            gt_filtered.append(0)\n",
    "        elif(gt[i][indx]==[1,0,0]):\n",
    "            gt_filtered.append(1)\n",
    "        elif(gt[i][indx]==[0,1,0]):\n",
    "            gt_filtered.append(2)\n",
    "        elif(gt[i][indx]==[0,0,1]):\n",
    "            gt_filtered.append(3)    \n",
    "\n",
    "classification_report(gt_filtered,pred_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "52dcdb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gt_filtered,pred_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c067431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.00      0.00      0.00         7\\n           1       0.00      0.00      0.00         6\\n           2       0.40      0.04      0.07        51\\n           3       0.00      0.00      0.00        23\\n           4       0.00      0.00      0.00        20\\n           5       0.70      0.15      0.25       104\\n           6       0.00      0.00      0.00        79\\n\\n   micro avg       0.64      0.06      0.11       290\\n   macro avg       0.16      0.03      0.05       290\\nweighted avg       0.32      0.06      0.10       290\\n samples avg       0.06      0.06      0.06       290\\n'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(pillar_labels_gt,pillar_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4a2dc540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06474820143884892"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pillar_labels_gt,pillar_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
