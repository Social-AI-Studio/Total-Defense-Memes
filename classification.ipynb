{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46572dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob2\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "import re\n",
    "import PIL\n",
    "from torchvision.models import vgg16\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89478a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"./annotation.json\",\"r\") as f:\n",
    "    annotation=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1186e97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Economic Defence': 155,\n",
       " 'Psychological Defence': 323,\n",
       " 'Social Defence': 43,\n",
       " 'Military Defence': 602,\n",
       " 'Digital Defence': 32,\n",
       " 'Others': 674,\n",
       " 'Civil Defence': 101}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a rough estimate of pillar-file count\n",
    "unique_pillars=[v for item in annotation[\"Pillar_Stances\"] for k,v in item.items()]\n",
    "unique_pillars=[item[0][0] for item in unique_pillars ]\n",
    "unique_pillars=list(set(unique_pillars))\n",
    "unique_pillars=[v for item in annotation[\"Pillar_Stances\"] for k,v in item.items()]\n",
    "unique_pillars=[item[0][0] for item in unique_pillars ]\n",
    "unique_pillars=list(set(unique_pillars))\n",
    "\n",
    "pillar_file_dict={}\n",
    "for item in annotation[\"Pillar_Stances\"]:\n",
    "    for k,items in item.items():\n",
    "        for itm in items:\n",
    "            if(len(set(itm[1]))==len(itm[1])):\n",
    "                continue\n",
    "            if(itm[0] not in pillar_file_dict):    \n",
    "                pillar_file_dict[itm[0]]=[k] \n",
    "            else:\n",
    "                pillar_file_dict[itm[0]].append(k) \n",
    "               \n",
    "{pillar:len(_) for pillar,_ in pillar_file_dict.items()}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81b53e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1155, 379, 377)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60-20-20 split\n",
    "train,val,test=[],[],[]\n",
    "for pillar,items in pillar_file_dict.items():\n",
    "    temp=np.random.choice(items,int(0.60*len(items)),replace=False)\n",
    "    train.extend(temp)\n",
    "    items=[item for item in items if item not in train]\n",
    "    temp=np.random.choice(items,int(0.5*len(items)),replace=False)\n",
    "    val.extend(temp)\n",
    "    items=[item for item in items if item not in val]\n",
    "    test.extend(items)\n",
    "len(train),len(val),len(test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "493df17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Economic Defence': 109,\n",
       "  'Digital Defence': 20,\n",
       "  'Military Defence': 372,\n",
       "  'Others': 404,\n",
       "  'Social Defence': 32,\n",
       "  'Psychological Defence': 219,\n",
       "  'Civil Defence': 61},\n",
       " {'Economic Defence': 32,\n",
       "  'Digital Defence': 6,\n",
       "  'Military Defence': 123,\n",
       "  'Others': 135,\n",
       "  'Social Defence': 10,\n",
       "  'Psychological Defence': 68,\n",
       "  'Civil Defence': 21},\n",
       " {'Economic Defence': 31,\n",
       "  'Digital Defence': 7,\n",
       "  'Military Defence': 117,\n",
       "  'Others': 135,\n",
       "  'Social Defence': 10,\n",
       "  'Psychological Defence': 64,\n",
       "  'Civil Defence': 24})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train split pillar-wise\n",
    "# file_pillar_dict={file:k for k,files in pillar_file_dict.items() for file in files}\n",
    "file_pillar_dict={}\n",
    "for k,files in pillar_file_dict.items():\n",
    "    for file in files:\n",
    "        if(file in file_pillar_dict):\n",
    "            file_pillar_dict[file].append(k)\n",
    "        else:\n",
    "            file_pillar_dict[file]=[k]\n",
    "            \n",
    "train_pillar_distr={pillar:0 for pillar in unique_pillars}\n",
    "\n",
    "for item in train:\n",
    "    for pillar in file_pillar_dict[item]:\n",
    "        train_pillar_distr[pillar]+=1\n",
    "        \n",
    "val_pillar_distr={pillar:0 for pillar in unique_pillars}\n",
    "for item in val:\n",
    "    for pillar in file_pillar_dict[item]:\n",
    "        val_pillar_distr[pillar]+=1\n",
    "        \n",
    "test_pillar_distr={pillar:0 for pillar in unique_pillars}\n",
    "for item in test:\n",
    "    for pillar in file_pillar_dict[item]:\n",
    "        test_pillar_distr[pillar]+=1   \n",
    "        \n",
    "train_pillar_distr,val_pillar_distr,test_pillar_distr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4935245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train upsample\n",
    "train_others=[item for item in train if 'Others' in file_pillar_dict[item]]\n",
    "train_economic=[item for item in train if 'Economic Defence' in file_pillar_dict[item]]\n",
    "train_social=[item for item in train if 'Social Defence' in file_pillar_dict[item]]\n",
    "train_digital=[item for item in train if 'Digital Defence' in file_pillar_dict[item]]\n",
    "train_psychological=[item for item in train if 'Psychological Defence' in file_pillar_dict[item]]\n",
    "train_civil=[item for item in train if 'Civil Defence' in file_pillar_dict[item]]\n",
    "train_military=[item for item in train if 'Military Defence' in file_pillar_dict[item]]\n",
    "\n",
    "train_upsample=[]\n",
    "train_upsample.extend(train_others)\n",
    "# train_upsample.extend(np.random.choice(train_others,315-236))\n",
    "\n",
    "train_upsample.extend(train_economic)\n",
    "train_upsample.extend(np.random.choice(train_economic,404-72))\n",
    "\n",
    "train_upsample.extend(train_social)\n",
    "train_upsample.extend(np.random.choice(train_social,404-14)) \n",
    "\n",
    "train_upsample.extend(train_digital)\n",
    "train_upsample.extend(np.random.choice(train_digital,404-16))\n",
    "\n",
    "train_upsample.extend(train_psychological)\n",
    "train_upsample.extend(np.random.choice(train_psychological,404-166))\n",
    "\n",
    "train_upsample.extend(train_civil)\n",
    "train_upsample.extend(np.random.choice(train_civil,404-55)) \n",
    "\n",
    "train_upsample.extend(train_military)\n",
    "train_upsample.extend(np.random.choice(train_military,404-348)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd198a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "pillars=[file for item in annotation[\"Pillar_Stances\"] for file,_ in item.items()]\n",
    "for item in annotation[\"Text\"]:\n",
    "    for file,text in item.items():\n",
    "        if(file not in pillars):\n",
    "            continue\n",
    "        data[file]=(text,\"./TD_Memes/{}\".format(file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d56f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Economic Defence': 0, 'Digital Defence': 1, 'Military Defence': 2, 'Others': 3, 'Social Defence': 4, 'Psychological Defence': 5, 'Civil Defence': 6} {'Economic Defence': [0, 1, 2], 'Digital Defence': [3, 4, 5], 'Military Defence': [6, 7, 8], 'Others': [9, 10, 11], 'Social Defence': [12, 13, 14], 'Psychological Defence': [15, 16, 17], 'Civil Defence': [18, 19, 20]}\n"
     ]
    }
   ],
   "source": [
    "pillar_labels_dict={pillar:i for i,pillar in enumerate(unique_pillars)}\n",
    "unique_stances=[pillar_stance for item in annotation[\"Pillar_Stances\"] \\\n",
    "                 for _,pillar_stance in item.items() if len(pillar_stance)==1]\n",
    "               \n",
    "unique_stances=set([stance for item in unique_stances for stance in item[0][1]])   \n",
    "unique_stances={stance:i for i,stance in enumerate(unique_stances)}\n",
    "stance_labels_dict={}\n",
    "index=0\n",
    "for pillar,_ in pillar_labels_dict.items():\n",
    "    stance_labels_dict[pillar]=[]\n",
    "    for stance in unique_stances:\n",
    "        stance_labels_dict[pillar].append(index)\n",
    "        index+=1\n",
    "print(pillar_labels_dict,stance_labels_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4b5c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pillar_stance={file:pillar_stance for item in annotation[\"Pillar_Stances\"] \\\n",
    "                 for file,pillar_stance in item.items()}\n",
    "file_pillar_stance={file:{item[0]:stance for item in pillar_stance if len(set(item[1]))<len(item[1])\\\n",
    "                          for stance in item[1] if item[1].count(stance)>1}\\\n",
    "                    for file,pillar_stance in file_pillar_stance.items()\\\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c7a68a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self,items):\n",
    "        self.items=items\n",
    "#         self.transform=transforms.Compose([\n",
    "#             transforms.Resize((224,224)),\n",
    "# #         transforms.CenterCrop((224,224)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#             ])\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self,index):        \n",
    "        with open(\"./vgg_embeddings/{}\".\\\n",
    "                  format(self.items[index]),\"rb\") as f:\n",
    "            encoding=pkl.load(f)\n",
    "        encoding=encoding.view(-1)    \n",
    "#         encoding=torch.cat((encoding['text_embeds'],encoding['image_embeds']),1)  \n",
    "        \n",
    "#         img=PIL.Image.open(self.items[index]['image_path']).convert('RGB')\n",
    "#         img=self.transform(img)\n",
    "#         encoding=encoding.view(-1,4096)\n",
    "#         encoding=resnet(img.unsqueeze(0)).view(-1,512)\n",
    "        pillar_labels=torch.zeros(7)  \n",
    "        stance_labels=torch.zeros(21)\n",
    "        for k,v in file_pillar_stance[self.items[index]].items():\n",
    "            pillar_labels[pillar_labels_dict[k]]=1\n",
    "            if(v==\"Supportive\"):\n",
    "                stance_labels[stance_labels_dict[k][0]]=1\n",
    "            elif(v==\"Neutral\"):\n",
    "                stance_labels[stance_labels_dict[k][1]]=1\n",
    "            elif(v==\"Against\"):\n",
    "                stance_labels[stance_labels_dict[k][2]]=1\n",
    "                 \n",
    "        return encoding,pillar_labels,stance_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "93af9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pillar_Stance_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pillar_Stance_Classifier, self).__init__()\n",
    "#         self.pretrained_model=VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
    "#         for param in self.pretrained_model.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         self.pillar_cls_layer = torch.nn.Sequential(nn.Linear(25088, 4096),nn.ReLU(),nn.Linear(4096, 512),\\\n",
    "#                                                    nn.ReLU(),nn.Linear(512,7))#\n",
    "#         self.stance_cls_layer = torch.nn.Sequential(nn.Linear(25088, 4096),nn.ReLU(),nn.Linear(4096, 512),\\\n",
    "#                                                    nn.ReLU(),nn.Linear(512,21))\n",
    "        \n",
    "        self.pillar_cls_layer = torch.nn.Sequential(nn.Linear(8*512, 64),nn.ReLU(),nn.Linear(64,7))#\n",
    "        self.stance_cls_layer = torch.nn.Sequential(nn.Linear(8*512, 64),nn.ReLU(),nn.Linear(64,21))\n",
    "\n",
    "    def forward(self,inp):\n",
    "#         output=self.pretrained_model(inp)\n",
    "        # for visualBERT\n",
    "#         output=output.last_hidden_state\n",
    "#         print(inp.shape)\n",
    "        output_pillar=self.pillar_cls_layer(inp)\n",
    "        output_stance=self.stance_cls_layer(inp)\n",
    "        return output_pillar,output_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "15e75f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model=Pillar_Stance_Classifier().to(device)\n",
    "model.train()\n",
    "train_set=ClassifierDataset(train_upsample)\n",
    "val_set=ClassifierDataset(val)\n",
    "train_loader=DataLoader(train_set, batch_size = 64)\n",
    "val_loader = DataLoader(val_set, batch_size = 64)   \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "08acb883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:46.44727572798729 validation loss:4.816993623971939\n",
      "training loss:35.94687816500664 validation loss:4.2128506898880005\n",
      "training loss:27.825812816619873 validation loss:4.093301296234131\n",
      "training loss:23.760528579354286 validation loss:3.922414720058441\n",
      "training loss:20.794313341379166 validation loss:3.8788132071495056\n",
      "training loss:18.481240831315517 validation loss:3.896069347858429\n",
      "training loss:16.747145354747772 validation loss:3.8111177384853363\n",
      "training loss:14.803797036409378 validation loss:3.817850649356842\n",
      "training loss:13.234844997525215 validation loss:3.8421100676059723\n",
      "training loss:12.025866065174341 validation loss:3.760339319705963\n",
      "training loss:10.84834874048829 validation loss:3.804943561553955\n",
      "training loss:9.825224686414003 validation loss:3.806150257587433\n",
      "training loss:8.939231241121888 validation loss:3.8802632689476013\n",
      "training loss:8.114565830677748 validation loss:3.976857304573059\n",
      "training loss:7.493706060573459 validation loss:3.978929877281189\n",
      "training loss:6.749775987118483 validation loss:4.094825387001038\n",
      "training loss:6.176387379877269 validation loss:4.0868958830833435\n",
      "training loss:5.546537450514734 validation loss:4.1665090918540955\n",
      "training loss:5.047231065109372 validation loss:4.221064209938049\n",
      "training loss:4.587997879832983 validation loss:4.317787826061249\n",
      "training loss:4.34386208280921 validation loss:4.320372462272644\n",
      "training loss:3.910384319256991 validation loss:4.389950186014175\n",
      "training loss:3.58932195417583 validation loss:4.526611626148224\n",
      "training loss:3.258455077186227 validation loss:4.543914318084717\n",
      "training loss:3.0163458734750748 validation loss:4.744501411914825\n",
      "training loss:2.6052351703401655 validation loss:4.700434148311615\n",
      "training loss:2.3088283464312553 validation loss:4.9838398694992065\n",
      "training loss:2.0840497470926493 validation loss:4.7633838057518005\n",
      "training loss:1.836021970026195 validation loss:5.124375283718109\n",
      "training loss:1.5825918787159026 validation loss:4.961754381656647\n",
      "training loss:1.3711329277139157 validation loss:5.262693226337433\n",
      "training loss:1.185397830675356 validation loss:5.0746142864227295\n",
      "training loss:1.0308619115967304 validation loss:5.377315998077393\n",
      "training loss:0.8455310936551541 validation loss:5.362907588481903\n",
      "training loss:0.7402853103121743 validation loss:5.511971592903137\n",
      "training loss:0.6377947793807834 validation loss:5.555236577987671\n",
      "training loss:0.5741319260559976 validation loss:5.675984084606171\n",
      "training loss:0.5059190536849201 validation loss:5.726377248764038\n",
      "training loss:0.46450796897988766 validation loss:5.820694327354431\n",
      "training loss:0.418159261578694 validation loss:5.874654293060303\n",
      "training loss:0.38665600738022476 validation loss:5.969288647174835\n",
      "training loss:0.3496764348819852 validation loss:5.996981978416443\n",
      "training loss:0.32125676854047924 validation loss:6.08586448431015\n",
      "training loss:0.29100281780119985 validation loss:6.127187728881836\n",
      "training loss:0.2669151569134556 validation loss:6.208465337753296\n",
      "training loss:0.24396977393189445 validation loss:6.254940450191498\n",
      "training loss:0.2243336812243797 validation loss:6.330674350261688\n",
      "training loss:0.2092426301096566 validation loss:6.375082850456238\n",
      "training loss:0.1935992069193162 validation loss:6.437683582305908\n",
      "training loss:0.1805945968371816 validation loss:6.483231782913208\n",
      "training loss:0.16819925082381815 validation loss:6.544986844062805\n",
      "training loss:0.1581587073742412 validation loss:6.591150760650635\n",
      "training loss:0.14778477477375418 validation loss:6.6435129046440125\n",
      "training loss:0.13955823206924833 validation loss:6.681748151779175\n",
      "training loss:0.12949327190290205 validation loss:6.748936951160431\n",
      "training loss:0.12343679400510155 validation loss:6.781894683837891\n",
      "training loss:0.11513995777931996 validation loss:6.846692740917206\n",
      "training loss:0.11002440244192258 validation loss:6.871403932571411\n",
      "training loss:0.10243914005695842 validation loss:6.933811664581299\n",
      "training loss:0.09861183213070035 validation loss:6.963367462158203\n",
      "training loss:0.09193609026260674 validation loss:7.020834505558014\n",
      "training loss:0.08813633988029324 validation loss:7.0479371547698975\n",
      "training loss:0.08222155095427297 validation loss:7.112269639968872\n",
      "training loss:0.07939164957497269 validation loss:7.142110824584961\n",
      "training loss:0.07445974947768264 validation loss:7.199714958667755\n",
      "training loss:0.071918411180377 validation loss:7.222590446472168\n",
      "training loss:0.06743860826827586 validation loss:7.27875554561615\n",
      "training loss:0.06503261037869379 validation loss:7.303236663341522\n",
      "training loss:0.06136913149384782 validation loss:7.358037710189819\n",
      "training loss:0.05907518326421268 validation loss:7.385034382343292\n",
      "training loss:0.05583990682498552 validation loss:7.439307451248169\n",
      "training loss:0.05350755201652646 validation loss:7.472876191139221\n",
      "training loss:0.050910909892991185 validation loss:7.517781496047974\n",
      "training loss:0.04898978963319678 validation loss:7.544719576835632\n",
      "training loss:0.04656734004674945 validation loss:7.5916144251823425\n",
      "training loss:0.04499175418459345 validation loss:7.613705039024353\n",
      "training loss:0.04260811922722496 validation loss:7.669871926307678\n",
      "training loss:0.04136670356092509 validation loss:7.690175652503967\n",
      "training loss:0.03931945885415189 validation loss:7.741226613521576\n",
      "training loss:0.038181150492164306 validation loss:7.762258410453796\n",
      "training loss:0.036268045820179395 validation loss:7.80534565448761\n",
      "training loss:0.03504609836090822 validation loss:7.8345730900764465\n",
      "training loss:0.03341095776704606 validation loss:7.8738903403282166\n",
      "training loss:0.03222556284163147 validation loss:7.906634628772736\n",
      "training loss:0.030728494471986778 validation loss:7.949848651885986\n",
      "training loss:0.02976353252597619 validation loss:7.977785885334015\n",
      "training loss:0.02845250887912698 validation loss:8.015449523925781\n",
      "training loss:0.027542324693058617 validation loss:8.042627573013306\n",
      "training loss:0.026456713465449866 validation loss:8.077127575874329\n",
      "training loss:0.025604287773603573 validation loss:8.104662001132965\n",
      "training loss:0.0245598507244722 validation loss:8.143313527107239\n",
      "training loss:0.023745963670080528 validation loss:8.176547884941101\n",
      "training loss:0.022855743984109722 validation loss:8.206194698810577\n",
      "training loss:0.0220977716599009 validation loss:8.242839395999908\n",
      "training loss:0.021251881269563455 validation loss:8.268881916999817\n",
      "training loss:0.020578786039550323 validation loss:8.304573059082031\n",
      "training loss:0.019723388249985874 validation loss:8.335749626159668\n",
      "training loss:0.019101277997833677 validation loss:8.366940200328827\n",
      "training loss:0.01840429745061556 validation loss:8.397921442985535\n",
      "training loss:0.017806911280786153 validation loss:8.434785008430481\n"
     ]
    }
   ],
   "source": [
    "val_losses=[]\n",
    "for _e in range(100):\n",
    "    train_loss=0\n",
    "    for t, (embedding, pillar_labels,stance_labels) in enumerate(train_loader):\n",
    "        embedding=embedding.to(device)\n",
    "#         input_ids=embedding['input_ids'].to(device)\n",
    "#         token_type_ids=embedding['token_type_ids'].to(device)\n",
    "#         attention_mask=embedding['attention_mask'].to(device)\n",
    "#         visual_embeds=embedding['visual_embeds'].to(device)\n",
    "        \n",
    "        pillar_labels =pillar_labels.to(device) \n",
    "        stance_labels =stance_labels.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pillar_logits,stance_logits=model(embedding)\n",
    "        pillar_loss = criterion(pillar_logits.squeeze(1), pillar_labels.float())\n",
    "        stance_loss = criterion(stance_logits.squeeze(1), stance_labels.float())\n",
    "        loss=pillar_loss+stance_loss\n",
    "        train_loss+=loss.data.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss= np.mean(train_loss)\n",
    "    val_loss=0\n",
    "    for t, (embedding,pillar_labels,stance_labels) in enumerate(val_loader):\n",
    "        embedding=embedding.to(device)\n",
    "        pillar_labels =pillar_labels.to(device) \n",
    "        stance_labels =stance_labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pillar_logits,stance_logits=model(embedding)\n",
    "        pillar_loss = criterion(pillar_logits.squeeze(1), pillar_labels.float())\n",
    "        stance_loss = criterion(stance_logits.squeeze(1), stance_labels.float())\n",
    "        val_loss+=(pillar_loss+stance_loss).data.item()\n",
    "    val_loss= np.mean(val_loss)   \n",
    "    if(len(val_losses)>0 and val_loss<min(val_losses)):\n",
    "          torch.save(model.state_dict(), 'model_vgg.pt')  \n",
    "    val_losses.append(val_loss)      \n",
    "    print('training loss:{} validation loss:{}'.format(train_loss,val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "77c01c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "model=Pillar_Stance_Classifier().to(device)\n",
    "test_set=ClassifierDataset(test)\n",
    "test_loader=DataLoader(test_set, batch_size = 1,shuffle=False)\n",
    "model.load_state_dict(torch.load(\"model_vgg.pt\"))\n",
    "model.eval()\n",
    "preds=[]\n",
    "pillar_outputs=[]\n",
    "stance_outputs=[]\n",
    "pillar_labels_gt=[]\n",
    "stance_labels_gt=[]\n",
    "stance_output_proba=[]\n",
    "filenames=[]\n",
    "with torch.no_grad():\n",
    "    for t, (embedding,pillar_labels,stance_labels) in enumerate(test_loader):\n",
    "        embedding=embedding.to(device)\n",
    "        pillar_labels =pillar_labels.to(device) \n",
    "        stance_labels =stance_labels.to(device)  \n",
    "        pillar_logits,stance_logits=model(embedding)\n",
    "        pillar_output,stance_output=torch.sigmoid(pillar_logits),torch.sigmoid(stance_logits)\n",
    "        temp=[0 if item<0.5 else 1 for item in pillar_output.view(-1).tolist()]\n",
    "        pillar_outputs.append(temp) \n",
    "        stance_outputs.append([0 if item<0.5 else 1 for item in stance_output.view(-1).tolist()])\n",
    "        stance_output_proba.append(stance_output.view(-1).tolist())\n",
    "        pillar_labels_gt.append(pillar_labels.view(-1).tolist())\n",
    "        stance_labels_gt.append(stance_labels.view(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d5bfe6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '           0       0.00      0.00      0.00        91\\n'\n",
      " '           1       1.00      0.18      0.31        11\\n'\n",
      " '           2       0.41      0.76      0.54        25\\n'\n",
      " '           3       0.24      0.93      0.39        28\\n'\n",
      " '\\n'\n",
      " '    accuracy                           0.30       155\\n'\n",
      " '   macro avg       0.41      0.47      0.31       155\\n'\n",
      " 'weighted avg       0.18      0.30      0.18       155\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "gt=[[item[n*3:(n+1)*3] for n in range(7)] for item in stance_labels_gt]\n",
    "stance_outputs=[[item[n*3:(n+1)*3] for n in range(7)] for item in stance_output_proba]\n",
    "\n",
    "pred_filtered=[]\n",
    "gt_filtered=[]\n",
    "for i,item in enumerate(pillar_outputs):\n",
    "    temp=np.array(item)\n",
    "    indices=[it for item in np.where(temp==1) for it in item]\n",
    "    for indx in indices:\n",
    "        arg=np.argmax(stance_outputs[i][indx])\n",
    "        \n",
    "        if(arg==0):\n",
    "            pred_filtered.append(1)\n",
    "        elif(arg==1):\n",
    "            pred_filtered.append(2)\n",
    "        elif(arg==2):\n",
    "            pred_filtered.append(3)\n",
    "        if(gt[i][indx]==[0,0,0]):\n",
    "            gt_filtered.append(0)\n",
    "        elif(gt[i][indx]==[1,0,0]):\n",
    "            gt_filtered.append(1)\n",
    "        elif(gt[i][indx]==[0,1,0]):\n",
    "            gt_filtered.append(2)\n",
    "        elif(gt[i][indx]==[0,0,1]):\n",
    "            gt_filtered.append(3)    \n",
    "\n",
    "pprint(classification_report(gt_filtered,pred_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c067431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '           0       0.50      0.13      0.21        31\\n'\n",
      " '           1       0.00      0.00      0.00         7\\n'\n",
      " '           2       0.51      0.19      0.28       117\\n'\n",
      " '           3       0.00      0.00      0.00       135\\n'\n",
      " '           4       1.00      0.30      0.46        10\\n'\n",
      " '           5       0.35      0.55      0.42        64\\n'\n",
      " '           6       0.00      0.00      0.00        24\\n'\n",
      " '\\n'\n",
      " '   micro avg       0.41      0.16      0.24       388\\n'\n",
      " '   macro avg       0.34      0.17      0.20       388\\n'\n",
      " 'weighted avg       0.28      0.16      0.18       388\\n'\n",
      " ' samples avg       0.15      0.15      0.15       388\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pprint(classification_report(pillar_labels_gt,pillar_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4a2dc540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14058355437665782"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pillar_labels_gt,pillar_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f3837a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3032258064516129"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gt_filtered,pred_filtered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
